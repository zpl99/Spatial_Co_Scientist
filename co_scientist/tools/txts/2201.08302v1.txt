PREPRINT

1

The R Package HCV for Hierarchical
Clustering from Vertex-links

by ShengLi Tzeng and Hao-Yun Hsu

Abstract The HCV package implements the hierarchical clustering for spatial data. It requires cluster-
ing results not only homogeneous in non-geographical features among samples but also geographically
close to each other within a cluster. We modiﬁed typically used hierarchical agglomerative clustering
algorithms to introduce the spatial homogeneity, by considering geographical locations as vertices
and converting spatial adjacency into whether a shared edge exists between a pair of vertices. The
main function HCV obeying constraints of the vertex links automatically enforces the spatial contiguity
property at each step of iterations. In addition, two methods to ﬁnd an appropriate number of clusters
and to report cluster members are also provided.

Introduction

Clustering analysis is a practical means to explore heterogeneity among subsets of the data. Clustering
algorithms aim to partition data into groups by maximizing the within-group homogeneity and
the between-group heterogeneity at the same time. When it comes to spatial data, however, we
need to carefully deﬁne what to cluster. This is because spatial data have two types of attributes:
geometrical attributes such as geographical coordinates, and non-geometrical features such as values
of temperature and rainfall. We shall refer to attributes on the former as the geometry domain and
the latter as the feature domain. In many applications, clustering results are required to take care of
information from both domains, and keeping the spatial contiguity within clusters is of great interest.

What makes things more complicated is that the analytical methods for spatial data are usually
taxonomized into three categories, namely, point-level or geostatistical data, areal or lattice data, and
point patterns (see Banerjee et al., 2003 or Cressie, 1993 ). We collect areal or lattice data when a
geometry domain is divided into several subregions at which non-geometrical features are aggregated.
Point-level or geostatistical data are collected at ﬁnite number of sites over the geometry domain for
certain spatial random variables. A point pattern consists of the characteristics of individual locations,
their variations and interactions in the geometry domain. Clustering of point patterns has been well
developed; see Kriegel et al. (2011) for a review, and we will not consider this kind of problem in what
follows. This article proposes a method that can be applied to the other two situations in the same
manner. We shall start from a brief review of existing works.

For clustering areal data, where each spatial unit is typically represented by a polygon, the R
package ClustGeo (Chavent et al., 2018) balances the proximities in geometry domain and feature
domain by a weighted sum of both. However, ClustGeo may ﬁnd clusters not necessarily having
spatial contiguity. To bypass this issue, we have to rely on the graph adjacency between vertices by
treating each spatial unit as a vertex. For example, Assunção et al. (2006) used minimum spanning
trees to identify strictly contiguous geographical regions with homogeneity on the feature domain.
Carvalho et al. (2009) modiﬁed usual agglomerative hierarchical clustering methods to achieve a
similar goal.

For clustering point-level spatial data, an often seen name is dual clustering, e.g., Zhou et al. (2007)
and Lin et al. (2005). It uses k-means or k-medoids clustering accounting for spatial proximities, but
the results sometimes lead to spatially fragmented clusters. Liao and Peng (2012) handled the issue
with local search, and Xie (2019) introduced constraints via Delaunay triangulations (Chew, 1989). The
R package SPODT (Gaudart et al., 2015) utilized oblique regression trees to partition the geometry
domain.

Although clustering of spatial data has many methodological and practical applications, methods
for different categories of data types seem to be developed independently. Very few point-level
clustering methods can be directly applied to areal data, while algorithms for areal data rarely consider
point-level cases. In this article, we present a package HCV in R that implements an algorithm to deal
with data in both types in a uniﬁed way. Methods for ﬁnding an appropriate number of clusters are
also provided.

The rest of the paper is organized as follows. In the next section, we explain the required input
formats and key subroutines in HCV. Next, we demonstrate the implemented functions in this package
using simulated data and a real social-economic dataset. Finally, Section Summary concludes the
paper.

2
2
0
2

n
a
J

3
1

]

O
C

.
t
a
t
s
[

1
v
2
0
3
8
0
.
1
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
PREPRINT

2

An overview of HCV functionality

We implemented a function called HCV as the fundamental agglomerative clustering for areal data,
which was also discussed in Carvalho et al. (2009), taking into account the constraints of vertex links
on the geometry domain. We will demonstrate that such agglomerative clustering can be also applied
to point-level data, through a intermediate tessellation. Therefore, we can analyze data in both types
with an identical framework. The resulting clusters are guaranteed to be spatially contiguous.

An important subject for practices of clustering is to determine the appropriate number of clusters.
The package also provides a function called getCluster with two methods to deal with the problem.

Data input structures

As mentioned above, spatial data clustering involves two domains. We describe the required input
formats for areal and point-level data in the following.

For areal data, we need to prepare a data.frame object whose columns stand for attributes on
the feature domain, and a SpatialPolygonsDataFrame object for deﬁning polygons on the geometry
domain. Here are simple codes to generate the two required objects, and the results are shown in
Figure 1. Note that the names of polygons should match the row names of the feature domain.

library(sp)
grd <- GridTopology(c(1,1), c(1,1), c(5,5))
polys <- as(grd, "SpatialPolygons")
centroids <- coordinates(polys)
gdomain <- SpatialPolygonsDataFrame(polys,

data=data.frame(x=centroids[,1], y=centroids[,2],
row.names=row.names(polys)))
fdomain <- data.frame(A=gdomain$x*5+gdomain$y^2,

B=cos(gdomain$x+gdomain$y),
C=sqrt(gdomain$x*gdomain$y))

The input format of geometry domain can be a adjacency matrix instead, which can be easily converted
from a SpatialPolygonsDataFrame object with gTouches in the package rgeos like this:

adj_mat <- rgeos::gTouches(gdomain,byid=TRUE)

For point-level data, two data.frame object are required for the feature domain and the ge-
ometry domain. Each row of the geometry domain object is the geographical coordinate for a
certain sample. We can convert these coordinates into a adjacency matrix based on a function
tessellation_adjacency_matrix. The adjacency matrix relies on Voronoi tessellation of the ge-
ometry domain, such that two samples are adjacent if their Voronoi cells share a common edge.
Below is an example of the conversion, and Figure 2 visualizes the result. For locations on a
three-dimensional geometry domain, a Delaunay tessellation can be applied similarly. The func-
tion tessellation_adjacency_matrix will give the adjacency matrix by automatically using a suitable
tessellation.

library(fields)
library(alphahull)
pts <- ChicagoO3$x
rownames(pts) <- LETTERS[1:20]
Vcells <- delvor(pts)
plot(Vcells,wlines='vor',pch='.')
text(pts,rownames(pts))
Amat <- tessellation_adjacency_matrix(pts)

Key subroutine: HCV algorithm

Before introducing the fundamental HCV clustering algorithm, we brieﬂy describe typical agglomerative
clustering without considering the information from the geometry domain. For i = 1, . . . , n, let
xi(si) = (xi1(si), . . . , xip(si))(cid:48) be a sample with p-dimensional attributes and si is its location center on
the geometry domain. The clustering algorithm starts by setting each sample as its single-element
cluster, and recursively merging the pair of clusters with the smallest dissimilarity. Euclidean distance

PREPRINT

3

Figure 1: A demonstration of inputs for areal data with a geometry domain (top-left) and three
attributes on the feature domain.

PREPRINT

4

Figure 2: A demonstration of a tessellation for point-level data (left) and a heatmap of the resulting
adjacent matrix, where the value of black squares is 1 (right).

is an often used dissimilarity for pairs of samples on their x values, i.e.,

d(x(si), x(sj)) =

(cid:32) p
∑
k=1

(cid:12)
(cid:12)
(cid:12)xik(si) − xjk(sj)

(cid:12)
(cid:12)
(cid:12)

2

(cid:33) 1
2

.

It is easy to see at the t-th iteration, there will be n − t + 1 candidate clusters among which a pair will
be merged. We denote these candidate clusters as {Ct
h, h = 1, . . . , n − t + 1}. A cluster may contain
more than one single point, and hence, several linkage methods for determining the between-cluster
dissimilarity has been proposed. The well known methods, including single, complete, average, and
Ward’s methods are all follow the Lance-Williams formula with different linear coefﬁcient (Lance and
Williams, 1967) in such a form:

(cid:16)

d

C

{t}
h

{t+1}
, C
k

(cid:17)

= αid
(cid:12)
(cid:12)
(cid:12)d

+ γ

(cid:16)

{t}
C
i
(cid:16)

C

{t}
h

(cid:17)

, C

{t}
h

(cid:17)

{t}
, C
i

+ αjd
(cid:16)

− d

(cid:16)

C

{t}
j

, C

{t}
h

(cid:17)

C

{t}
h

, C

{t}
j

(cid:17)(cid:12)
(cid:12)
(cid:12) ,

+ βd

(cid:16)

{t}
C
i

, C

{t}
j

(cid:17)

(1)

{t+1}
where in the (t + 1)-th iteration C
.
k
One applies a selected linkage method recursively to compute dissimilarity, and merge the closest pair
of clusters, to ﬁnd a hierarchy of clusters until all samples are in a single cluster.

{t}
is the cluster aggregated from cluster C
i

and cluster C

{t}
j

The idea of our HCV algorithm is generally based on the common edges of units on the geometry
diagram. If any pairs of units have a common edge in the Voronoi diagram, then they are considered
to have spatial proximity and geometry connectedness. In each iteration, the major difference between
HCV and a typical hierarchical agglomerative clustering algorithm is that, we only focus on the
geometry-connected groups. The next two candidate groups to be aggregated must share a common
{t}
hk = 1,

{t}
edge. That is, a pair of clusters C
and C
k
where the stepwise updated adjacency matrix A{t} is recursively deﬁned as

can be merged in the (t + 1)-th iteration only if A

{t}
h

A

{0}
ij =

(cid:40)

0,
1,

if i = j or the i-th and j-th smaples have no common edge;
if the i-th and j-th samples have a common edge;

A

{t}
hk =




0,



1,

if A

if A

{t−1}
hj
{t−1}
hj

= 0 and A

= 0,

= 1 or A

= 1.

{t−1}
hi
{t−1}
hi

(2)

(3)

Putting these ideas altogether, the algorithm details of the implemented HCV function are given in
Algorithm 1.

There are several linkage algorithms have been provided in HCV, including "ward", "single",
"complete", "average", "weight", "median", and "centroid". Default is ’ward’. Current available

PREPRINT

5

methods in HCV for the dissimilarity of the feature domain are "euclidean", "correlation", "abscor",
"maximum", "manhattan", "canberra", "binary" and "minkowski". Default is ’euclidean’.

Algorithm 1 Hierarchical Clustering from Vertex-links

(cid:110)

(cid:111)

{0}
C
i

; i = 1, . . . , n

1: Input geometry_domain, feature_domain, and linkage.
2: Set A{0} ≡ the adjacency matrix for geometry_domain using (2).
3: Calculate D{0} ≡ the dissimilarity matrix for feature_domain.
4: Set
5: Set t := 0.
6: Create T as an edgeless graph for the initial tree structure.
7: Create Tree.Height as an empty array.
8: while any off-diagonal element of A{t} is nonzero do
{t}
{t}
j
ij

{t}
Aggregate C
i

and C

D

9:

into CNEXT if arg min
i,j,Aij=1

with each sample being a single-element cluster.

10:
11:

12:

13:

14:

Set t := t + 1.
Update D{t} based on (1) according to the input linkage.
Update A{t} based on (3).
Update Tree.Height[t] = d

(cid:17)

(cid:16)

{t}
.
j
{t+1}
Add CNEXT into T as a new node C
k
{t+1}
Draw edges from C
k

{t}
C
i

, C

.

15:
16: end while
17: return Tree.Height and T containing list of clusters and the hierarchy

{t}
to its left and right nodes, i.e., C
i

and C

{t}
j

.

Key subroutine: Determining number of clusters

Another crucial issue in cluster analysis is to determine an appropriate number of clusters. There
are several internal indices and external indices; see e.g. Desgraupes (2018). However, these indices
consider only the within-cluster sum of squared difference of attributes on the feature domain, which
overlooks the information on the geometry domain. In view of this, we propose a novel internal index
named Spatial Mixture Index (SMI) for determining the number of clusters.

In some applications, having the number of clusters is not satisfactory enough. We may want to
know the stability of clusters and their adherent members. To this end, we provide another method
‘M3C’ based on Monte Carlo simulations to ﬁnd stable clusters.

Both ‘SMI’ and ‘M3C’ are available methods in the wrapper function getCluser, which provides

users a more convenient tool for the number of clusters.

SMI

By connecting the pairs of points sharing common edges on the geometry domain, we have many
paths between samples. Denote (cid:96)ij the minimum number of paths through which si can reach sj.
SMI is an internal index involving the property of spatial proximity by considering the edge length
of polygons or of triangles after a tessellation. SMI is deﬁned in (4), where ∆
k and ek represent the
average squared distance within the k-th cluster for the feature domain and the geometry domain,
respectively. The overall heterogeneity is a weighted sum of both squared distances for all clusters,
with weights depending on f (·).

SMI(K) =

1
K

K
∑
k=1

[ f (αk)∆

k + (1 − f (αk))ek] ,

(4)

PREPRINT

where

6

∆

k =

ek =

1
|Ck|2 − |Ck|
ρ
|Ck|2 − |Ck|

∑
i∈Ck
∑
i∈Ck

∑
j∈Ck\i
∑
j∈Ck\i

d(xi(si), xj(sj))2,

(cid:96)2
ij,

∑n

i=1

ρ =

∑i−1

j=1 d(xi(si), xj(sj))2
∑n
j=1 (cid:96)2
ij

∑i−1

i=1

,

and αk and the function f are deﬁned as follows:

αk =

j=1 ej

Kek − ∑K
∑K
j=1 ej

,

f (x) = (1 + e−x)−1.
The squared distances are analogous to within-cluster sum of squares, and ρ deals with potiential
k and ek. We suggest to use K∗ = arg minKSMI(K)/SMI(K − 1) for selecting
different scales between ∆
the number of clusters. Our getCluster fucntion has a method argument. If method = ‘SMI’ is
speciﬁed, values of SMI from 2 to Kmax clusters will be compared, and individual cluster members are
reported, under the selected number of clusters based on SMI.

M3C

We modiﬁed a method in the M3C package (John et al., 2018) to take the uncertainty of cluster into
account via Monte Carlo simulations and calculating the consensus matrix. M3C compares the result
of the clusters at hand with the clusters generated by simulations, and it determines the appropriate
number of clusters based on the most signiﬁcant proportion of ambiguous clustering (PAC) score. As
its design is not for spatial data clustering, M3C does not consider the information on the geometry
domain. We construct a geometry embedded dissimilarity matrix (GEDM) to ﬁll the gap, and the
procedure is explained in the following.

Our method utilizes the output of the HCV algorithm, which carries information of the dissimilarity
between clusters and the constraints on the geometry. We turn such information into a GEDM through
Algorithm 2. Then the GEDM is converted into Euclidean coordinates using a metric multidimensional
scaling, followed by applying Monte Carlo reference-based consensus clustering of the M3C package.
The default score to evaluate cluster stability is PAC score, but users can specify an alternative score in
the criterion argument when necessary.

Similar to that with method = ‘SMI’, if method = ‘M3C’ is used in the implemented getCluster
fucntion, individual cluster members are reported, under the selected number of clusters having the
smallest score.

Algorithm 2 Geometry Embedded Dissimilarity Matrix
1: Input an HCV object with Tree.Height and T .
2: Set Imax the length of Tree.Height in the input HCV object
3: Create a matrix M with zero diagonals and all off-diagonals being Tree.Height[Imax].
4: Set GEDM := M.
5: for t from Imax to 1 do
6:

{t}
LeftMembers := members in LeftNode of C
k

in T
{t}
RightMembers := members in RightNode of C
k
for (cid:96) in LeftMembers do

in T

for r in RightMembers do

GEDM[(cid:96), r] = min(Tree.Height[t], GEDM[(cid:96),r])
GEDM[r, (cid:96)] = GEDM[(cid:96), r]

end for

7:

8:
9:
10:
11:
12:

end for

13:
14: end for
15: return GEDM

PREPRINT

7

Simulation Examples and Real Data

Simulation data

We used the way to generate synthetic point-level data propose by Lin et al. (2005) as follows. The
procedure is also implemented as the synthetic_data function in our package.

1. Specify the number of clusters (k), number of points (n), the dimension of feature domain (p1)

and geometry domain (p2).

2. Select exactly k points from the domain (0, 1)p1 randomly as the center of the k clusters on

feature domain, i.e., µ

f eat
1

, . . . , µ

f eat
k

.

3. Select exactly k points from the domain (0, 1)p2 randomly as the center of the k clusters on

geometry domain, i.e., µ

, . . . , µ

geo
1

geo
k

4. Draw a random point o from joint uniform distribution over the geometry domain.
i )− f
5. Assign point o to Ci by the probability mass function (PMF) as P(o ∈ Ci) = d(o,µgeo
o,µgeo
j

j=1 d

∑k

(cid:16)

(cid:17)− f

6. Draw a sample form joint normal distribution N(µ

the point o in Step 4.

f eat
i

, r2 I) as the feature domain attribute for

7. Repeat Step 4 to 6 until n points have been sampled.

f eat
In the above process, C
i

are the centers of feature domain and geometry domain of the
cluster Ci respectively. It is easy to see that the parameter f controls the degree of spatial homogeneity
in step 5. the larger f value presents, the more spatial homogeneity has been introduced, and we
controls the variation of the feature domain attribute by parameter r.

geo
, C
i

Here we present an example for clustering of point-level data. The parameters of synthetic data

were (k, f , r, n, p1, p2) = (3, 30, 0.02, 300, 2, 2).

set.seed(0)
pcase <- synthetic_data(3,30,0.02,300,2,2)
labcolor <- (pcase$labels+1)%%3+1
par(mfrow = c(1,2))
plot(pcase$feat, col = labcolor, pch=19, xlab = 'First attribute',

ylab = 'Second attribute', main = 'Feature domain')

plot(pcase$geo, col = labcolor, pch=19, xlab = 'First attribute',

ylab = 'Second attribute', main = 'Geometry domain')

Figure 3: The feature domain and the geometry domain of the simulated point-level data, where
distinct colors stand for different clusters.

PREPRINT

8

Figure 4: The cluster dendrogram for the simulated data on Figure 3 using HCV.

The simulated data are shown in Figure 3. We then compare the results from our package using
method=’SMI’ with that from the SPODT package. The detailed codes are given below. Since HCV
outputs an hclust object, we can easily plot the dengrogram as Figure 4shown, and use cutree to form
ﬁnal clusters. However, a closer examination will reveal that crossovers or inversions occur on such a
dendrogram, and hence cutree should be applied with care. We recommend getCluster instead for
determining clusters. The found clusters are shown in Figure 5. As can be seen, the proposed method
almost recovers the true clusters, while there are more mismatches based on SPODT.

# clustering with HCV
HCVobj <- HCV(pcase$geo, pcase$feat)
smi <- getCluster(HCVobj,method="SMI")
plot(HCVobj)

# clustering with SPODT
library(cluster)
library(SPODT)
grp <- pam(pcase$feat,k=3)
dataset <- data.frame(pcase$geo,grp$clustering)
colnames(dataset) <- c('x1','x2','grp')
coordinates(dataset) <- c("x1","x2")
spc <- spodt(dataset@data$grp~1, dataset, graft = 0.3)

# plots for results
par(mfrow=c(2,2))
plot(pcase$feat, col=factor(smi),pch=19, xlab = 'First attribute',

ylab = 'Second attribute',main = 'Feature domain')

plot(pcase$geo, col=factor(smi),pch=19, xlab = 'First attribute',

ylab = 'Second attribute',main = 'Geometry domain')

plot(pcase$feat, col=factor(spc@partition),pch=19, xlab = 'First attribute',

ylab = 'Second attribute',main = 'Feature domain')

plot(pcase$geo, col=factor(spc@partition),pch=19, xlab = 'First attribute',

ylab = 'Second attribute',main = 'Geometry domain')

Real application: French municipalities data

We used the estuary data in the ClustGeo package, which involved 303 France cities located along
Gironde estuary. There are four attributes on the feaure domain: city employee rate, city graduation
rate, ratio of apartment housing and agricultural area, and these are areal data on the geometry

PREPRINT

9

Figure 5: The scatterplot with found clusters using the HCV algorithm (top panel) and using the
SPODT package (bottom panel), where distinct colors stand for different clusters in each method.

PREPRINT

10

domain. For understanding the spatial distribution of the four features, we visualize via heatmaps
and represent the magnitude of standardized features in Figure 6. It is obvious that there are several
potential outliers in city employee rate and ratio of apartment housing that have extremely large
values.

library(ClustGeo)
library(fields)
data(estuary)
colors <- tim.colors(303)
lim <- c(-2.5,2.5)
dat <- estuary$dat
plotMap(estuary$map, scale(dat[,1]), color=colors, zlim=lim, bar='employ')
plotMap(estuary$map, scale(dat[,2]), color=colors, zlim=lim, bar='grad')
plotMap(estuary$map, scale(dat[,3]), color=colors, zlim=lim, bar='housing')
plotMap(estuary$map, scale(dat[,4]), color=colors, zlim=lim, bar='agriland')

In this example, we present to input a user-speciﬁed dissimilarity matrix and adjacency matrix,
by setting diss = ’precomputed’ and adjacency = TRUE as the codes given below. We use squared
Euclidean distance as the dissimilarity, and covert the SpatialPolygonsDataFrame object estuary$map
to an adjacency matrix. After calling the HCV function, we apply the getCluster function with
method=’M3C’ to ﬁnd the most stable clusters. We also compare it with the clusters found by ClustGeo.
The result are shown in Figure 7. Some cluster patterns are quite similar in the two methods, although
ClustGeo has more spatially fragmented clusters. If spatial contiguity is of interest, the HCV algorithm
is preferred.

# clustering with HCV
feat <- as.matrix(scale(dat))
fdist <- as.matrix(dist(feat)) ** 2
geo <- estuary$map
adj <- rgeos::gTouches(geo, byid=TRUE) * 1
tree1 <- HCV(adj, fdist,
m3c <- getCluster(tree1, method='M3C')

adjacency = T, diss = 'precomputed')

# clustering with ClustGeo
D0 <- dist(scale(dat))
D1 <- as.dist(1-adj)
tree2 <- hclustgeo(D0,D1,alpha=0.4)
cg <- cutree(tree2,7)

# plots for results
par(mfrow=c(1,2),mar=c(0,0,0,0))
plot(geo, col = tim.colors(7)[m3c], border='grey')
plot(map,border="grey",col=tim.colors(7)[cg])

Summary

In this paper, the R package HCV is presented for spatial data clustering with the contiguity property.
We modiﬁed hierarchical agglomerative clustering algorithms and methods for determining the
number of clusters, such that the spatial information on the geometry domain can be reasonablly
incorproated. We demonstrate the package applications to point-level data and areal data. By these
examples, we show that HCV is a uniﬁed framework for clustering both types of spatial data.

Acknowledgements

This research was supported in part by the Ministry of Science and Technology, Taiwan under Grants
110-2628-M-110-001-MY3. We also thank Yung-Jie Ye for part of the initial code prototype.

PREPRINT

11

(a) City employee rate

(b) City graduation rate

(c) Ratio of apartment housing

(d) Agricultural area

Figure 6: Heatmaps for standardized values of the four socio-economic variables in the estuary data.

Figure 7: The found clusters using the HCV algorithm (left) and using the ClustGeo package (right),
where distinct colors stand for different clusters in each method.

PREPRINT

12

Bibliography

R. M. Assunção, M. C. Neves, G. Câmara, and C. da Costa Freitas. Efﬁcient regionalization techniques
International Journal of

for socio-economic geographical units using minimum spanning trees.
Geographical Information Science, 20(7):797–811, 2006. [p1]

S. Banerjee, B. P. Carlin, and A. E. Gelfand. Hierarchical modeling and analysis for spatial data. Chapman

and Hall/CRC, 2003. [p1]

A. X. Y. Carvalho, P. H. M. Albuquerque, G. R. de Almeida Junior, and R. D. Guimaraes. Spatial

hierarchical clustering. Revista Brasileira de Biometria, 27(3):411–442, 2009. [p1, 2]

M. Chavent, V. Kuentz-Simonet, A. Labenne, and J. Saracco. Clustgeo: an r package for hierarchical

clustering with spatial constraints. Computational Statistics, 33(4):1799–1822, 2018. [p1]

L. P. Chew. Constrained delaunay triangulations. Algorithmica, 4(1):97–108, 1989. [p1]

N. Cressie. Statistics for spatial data. John Wiley & Sons, Hoboken, NJ, revised edition, 1993. [p1]

B. Desgraupes. clusterCrit: Clustering Indices, 2018. URL https://CRAN.R-project.org/package=

clusterCrit. R package version 1.2.8. [p5]

J. Gaudart, N. Graffeo, D. Coulibaly, G. Barbet, S. Rebaudet, N. Dessay, O. K. Doumbo, and R. Giorgi.
SPODT: An R package to perform spatial partitioning. Journal of Statistical Software, 63(16):1–23,
2015. URL http://www.jstatsoft.org/v63/i16/. [p1]

C. R. John, D. Watson, M. Lewis, D. Russ, K. Goldmann, M. Ehrenstein, C. Pitzalis, and M. Barnes.
M3c: A monte carlo reference-based consensus clustering algorithm. bioRxiv, 2018. doi: https:
//doi.org/10.1101/377002. [p6]

H.-P. Kriegel, P. Kröger, J. Sander, and A. Zimek. Density-based clustering. Wiley Interdisciplinary

Reviews: Data Mining and Knowledge Discovery, 1(3):231–240, 2011. [p1]

G. N. Lance and W. T. Williams. A general theory of classiﬁcatory sorting strategies: 1. hierarchical

systems. The computer journal, 9(4):373–380, 1967. [p4]

Z.-X. Liao and W.-C. Peng. Clustering spatial data with a geographic constraint: exploring local search.

Knowledge and information systems, 31(1):153–170, 2012. [p1]

C.-R. Lin, K.-H. Liu, and M.-S. Chen. Dual clustering: integrating data clustering over optimization
and constraint domains. IEEE Transactions on Knowledge and Data Engineering, 17(5):628–637, 2005.
[p1, 7]

S. Xie. Deﬁning geographical rating territories in auto insurance regulation by spatially constrained

clustering. Risks, 7(2):42, 2019. [p1]

J. Zhou, J. Guan, and P. Li. Dcad: a dual clustering algorithm for distributed spatial databases.

Geo-spatial Information Science, 10(2):137–144, 2007. [p1]

ShengLi Tzeng
Department of Applied Mathematics
National Sun Yat-sen University
Kaohsiung, Taiwan, 80424
slt.cmu@gmail.com

Hao-Yun Hsu
Department of Applied Mathematics
National Sun Yat-sen University
Kaohsiung, Taiwan, 80424
zoez5230100@gmail.com

