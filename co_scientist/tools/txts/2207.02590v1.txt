MetroGAN: Simulating Urban Morphology with Generative
Adversarial Network
Yiyang Ma
Wangxuan Institute of Computer
Technology, Peking University
Beijing, China
myy12769@pku.edu.cn

Di Zhu
Department of Geography,
Environment and Society, University
of Minnesota, Twin Cities
Minneapolis, USA
dizhu@umn.edu

Weiyu Zhang
Institute of Remote Sensing and
Geographic Information Systems,
School of Earth and Space Sciences,
Peking University
Beijing, China
wyzhang929@gmail.com

2
2
0
2

l
u
J

6

]

Y
C
.
s
c
[

1
v
0
9
5
2
0
.
7
0
2
2
:
v
i
X
r
a

Lei Dong
Institute of Remote Sensing and
Geographic Information Systems,
School of Earth and Space Sciences,
Peking University
Beijing, China
arch.dongl@gmail.com

Yu Liu∗
Institude of Remote Sensing and
Geographic Information Systems,
School of Earth and Space Sciences,
Peking University
Beijing, China
liuyu@urban.pku.edu.cn

ABSTRACT
Simulating urban morphology with location attributes is a challeng-
ing task in urban science. Recent studies have shown that Genera-
tive Adversarial Networks (GANs) have the potential to shed light
on this task. However, existing GAN-based models are limited by
the sparsity of urban data and instability in model training, ham-
pering their applications. Here, we propose a GAN framework with
geographical knowledge, namely Metropolitan GAN (MetroGAN),
for urban morphology simulation. We incorporate a progressive
growing structure to learn hierarchical features and design a geo-
graphical loss to impose the constraints of water areas. Besides, we
propose a comprehensive evaluation framework for the complex
structure of urban systems. Results show that MetroGAN outper-
forms the state-of-the-art urban simulation methods by over 20%
in all metrics. Inspiringly, using physical geography features singly,
MetroGAN can still generate shapes of the cities. These results
demonstrate that MetroGAN solves the instability problem of pre-
vious urban simulation GANs and is generalizable to deal with
various urban attributes.

CCS CONCEPTS
• Applied computing → Sociology; Arts and humanities; • Com-
puting methodologies → Computer vision.

∗Contact author.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
KDD ’22, August 14–18, 2022, Washington, DC, USA.
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9385-0/22/08. . . $15.00
https://doi.org/10.1145/3534678.3539239

KEYWORDS
Urban Morphology Simulation; Generative Adversarial Networks

ACM Reference Format:
Weiyu Zhang, Yiyang Ma, Di Zhu, Lei Dong, and Yu Liu. 2022. MetroGAN:
Simulating Urban Morphology with Generative Adversarial Network. In
Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’22), August 14–18, 2022, Washington, DC, USA. ACM,
New York, NY, USA, 11 pages. https://doi.org/10.1145/3534678.3539239

1 INTRODUCTION
The dynamics of cities influences many fields of our society, in-
cluding economic growth, climate change, and sustainable devel-
opment [6]. To study urban dynamics, morphology or the spatial
configuration is the key dimension [4, 5], which not only provides
a delineative representation of cities, but also is closely related
to the complex characteristics of cities, such as the fractality, the
polycentricity, and the scaling laws [5, 21].

Among relevant studies on urban morphology, simulating the
spatial structure of cities is one critical task. Existing work ad-
dresses this problem in two main ways. On the one hand, stud-
ies represented by statistical physics [34] or complexity science
(such as cellular automata [24]) usually model city growth from a
bottom-up perspective [34]. These models can capture key growth
mechanisms of cities, but the generated form is often very dif-
ferent from the real situation because of ignoring many complex
high-dimensional features of urban morphology [24]. On the other
hand, machine learning methods, especially deep generative mod-
els, have been validated owning the ability to approximate com-
plicated, high-dimensional probability distributions [13] and have
been widely used in urban science and geoscience in recent years
[30, 36]. Among deep generative models, the Generative Adver-
sarial Network (GAN) is proved to be a suitable method to study
spatial effects of geographical systems [1, 2]. Specifically, generat-
ing urban morphology is similar to the task of image generation,
which has been widely investigated in the GAN model family. How-
ever, the main problem in the urban setting is the severely sparse

 
 
 
 
 
 
KDD ’22, August 14–18, 2022, Washington, DC, USA.

Weiyu Zhang et al.

data (pixels with zero or low values account for a large proportion),
which makes models hard to train. The performances of previous
GAN-based models are limited by such data characteristics and face
instability problems.

An important way to solve the problem of data shortage and
improve the performance of the model is to incorporate geograph-
ical knowledge into models [18]. Based on this idea, we design a
GAN-based model for urban morphology simulation, namely the
Metropolitan GAN (MetroGAN). Considering the hierarchical and
self-similar characteristics of urban systems, we introduce progres-
sive growing structure into our model. This structure can gradually
learn the urban features at different scales and significantly stabilize
the model. Besides, we design a geographical loss to approximate
the constraint that urban areas can not be on the water, making
our model easier to learn the impacts of water areas.

To generate cities with MetroGAN, we collected the urban built-
up area map (one commonly used proxy of urban morphology) as
labels in our experiments. For inputs, we build a global city dataset
using three layers: terrain [digital elevation model (DEM)], water,
and nighttime lights (NTL). The first two are location’s physical
geography characteristics, which might dominate the growth of
cities [7]. NTL is a good proxy of socioeconomic development and
has been recognized to be helpful in city extracting tasks [8, 22].
Based on these three layers, we detect over 10,000 cities worldwide,
and each city is represented as a 100km × 100km image, covering
the central urban districts and surrounding settlements.

Since a city is a complex system with plenty of structural and
hierarchical features, we also propose an evaluation framework to
assess the generated cities. Conventional evaluations always use
pixel-by-pixel accuracy, which omit the connections among pixels
and therefore are not sufficient to measure urban morphology. Our
evaluation framework consists of four levels: pixel level, multi-scale
spatial level, perceptual level, and macroscopic level. The first three
levels are proposed for comparison. The multi-scale spatial level
is added to consider the spatial structure of the urban system, and
the perceptual level aims to compare the underlying structured
information of urban form. The macroscopic level is for validation
using geographical characteristics and laws.

Results show that using NTL, terrain, and water area as inputs,
our model can generate precise urban morphology. The generated
cities improve around 20% performance in terms of four metrics
listed in Section 5 compared with state-of-art models. Furthermore,
with physical geography characteristics singly, our model can still
generate rough shapes of cities.

Our model has a wide range of applications in urban science.
For example, by learning the mappings from attributes to mor-
phology, our model could help researchers explore the relations
between location attributes and urban morphology. Further, our
model provides a powerful tool for urban data augmentation. We
can reconstruct urban change over a longer time series with limited
snapshots.

To summarize, this research mainly has four contributions:

• We introduce MetroGAN, a model to simulate cities. The model
incorporates the urban morphology with progressive growing
structures, and physical geography constraints with designed
geographical loss.

• Our model can generate cities’ shapes just with physical geogra-
phy characteristics, shedding light on how and to what extent
nature environment constraints could affect the cities.

• We propose a comprehensive evaluation framework for assessing
urban morphology, which can be used for general city extraction,
prediction, and simulation tasks.

• We publish a dataset, including over 10,000 cities worldwide and
four types of urban data (DEM, water area, NTL, and built-up
area).

2 BACKGROUND AND RELATED WORK
2.1 Urban Morphology Generation
Various factors shape urban morphology, including geographical sit-
uations, initial settlements, technological developments, economic
activities, and others. Among them, physical geography (e.g., ter-
rains and water bodies) and socioeconomic activity (e.g., nighttime
lights) are important indicators.

Obviously, the terrains and water impact the cities’ morphology
[7], yet using them to simulate cities is very difficult. For example,
terrains have many derivatives, such as the slope, orientation, and
ruggedness (the degree to which the terrain is uneven). But as yet,
it is not known to what extend these features could affect cities.
This is also true for water areas, which are usually used as masks in
urban morphology simulation but are not considered as a driving
factor of cities.

Compared with terrains and water, NTL data is a more widely
used proxy to extract urban areas [8, 22]. Two main NTL data
sources are DMSP-OLS (Defense Meteorological Satellite Program’s
Operational Linescan System) [11] and NPP-VIIRS (Suomi National
Polar-orbiting Partnership satellite with the Visible Infrared Imag-
ing Radiometer Suite sensor) [10]. Based on NTL, various methods
have been proposed to extract cities, including the threshold method
[8] and the Random Forest model [15]. These methods classify each
pixel as an urban area or a non-urban area independently. In addi-
tion, the classification-based methods can only extract the rough
shape without the detailed structure of a city [22].

2.2 GANs Applied in Geospatial Domain
GAN has been demonstrated to be a powerful tool to fit high-
dimensional complex distributions [13]. This advantage makes GAN
good at modeling complex geospatial data with ubiquitous spatial
effects (e.g., spatial dependence and heterogeneity). Zhu et al. pro-
posed CEDGANs to interpolate spatial data in geographical context,
considering the spatial dependency effects [36] . Ravuri et al. com-
bined GAN and RNN to incorporate both spatial and temporal
dependencies [30].

GAN has also been used to generate cities. Albert et al. proposed
a GAN model to learn urban morphology and generate fake city
images that are both visually and statistically like real cities [2].
Furthermore, they use conditional GAN to synthesize urban built-
up maps using population distribution map, nightlight image, and
water area map [1]. However, their model is limited by the sparse
data and instability problems.

The above-mentioned GAN applications give a consistent body
of evidence that GAN is good at learning the complex mapping

MetroGAN: Simulating Urban Morphology with Generative Adversarial Network

KDD ’22, August 14–18, 2022, Washington, DC, USA.

between different geographical domains and considering spatial
effects and structural connections in geo-spatial systems.

3 METHODOLOGY
3.1 Problem Formulation
To introduce the urban simulation task to GANs, we represent the
urban morphology as distribution maps and formalize the task as
domain transfer of images. Specifically, transfer images from the
source domain, the various aspects of urban attributes, to the target
domain, the urban morphology. For the images of cities, we clipped
them representing 100km × 100km spatial extent and resized each
image to 256× 256 or 128× 128. Each pixel corresponds to around
390m × 390 m or 780m ×780m geographical extent.

The input images can be formulated as X = {x1, · · · , x𝑛 }, where
x𝑖 ∈ R𝑁 ×𝑁 is an image of source domains with size 𝑁 × 𝑁 . In this
work, the sources of 𝑥𝑖 can be DEM, NTL, and water area maps.
The output is defined as s ∈ R𝑁 ×𝑁 , which is a binary urban map (1
and 0 represent urban and non-urban areas respectively), and the
goal of our model is to learn the conditional distribution 𝑝 (s|X).
The challenge of the learning process is twofold: how to capture the
spatial effects and how to incorporate the geographical constraints.

3.2 Geographical Domain Transfer with

MetroGAN

In general, a GAN consists of two networks, a generator (G) and a
discriminator (D). D is trained to distinguish real images and fake
images, and G is trained to confuse the discriminator. G and D
compete with each other and finally reach an optimal equilibrium.
Conditional GAN (CGAN) [16] applies GANs in the conditional
setting and makes the generation process of G more controllable
to solve complex generation tasks. Based on CGAN, pix2pixGAN
[16] introduced GAN for image translation problems, which is a ge-
netic approach to problems that traditional image translation tasks
require specifically designed loss formulations. For our proposed
model, the inputs of G are {𝑥𝐴, 𝑧}, where 𝑥𝐴 is an image in domain
A, and 𝑧 is a random vector in latent space, and the inputs of D
are {𝑥𝐴, 𝑦} and {𝑥𝐴, 𝐺 (𝑥𝐴, 𝑧)}, where 𝑦 is the label image in target
domain and 𝐺 (𝑥𝐴, 𝑧) is the output image of G. D not only distin-
guishes whether the input is a "real" image, but also distinguishes
whether the input pair matches. The optimization objective of the
original pix2pixGAN is
𝐺 ∗ = arg min
𝐺

L𝑐𝐺𝐴𝑁 (𝐺, 𝐷) + 𝜆L𝐿1 (𝐺).

max
𝐷

(1)

L𝑐𝐺𝐴𝑁 (𝐺, 𝐷) =E𝑥,𝑦 [𝑙𝑜𝑔𝐷 (𝑥, 𝑦)]+

E𝑥,𝑧 [𝑙𝑜𝑔(1 − 𝐷 (𝑥, 𝐺 (𝑥, 𝑧)))],

(2)

L𝐿1 (𝐺) = E𝑥,𝑦,𝑧 [∥𝑦 − 𝐺 (𝑥, 𝑧)∥1],
(3)
where L𝑐𝐺𝐴𝑁 (𝐺, 𝐷) is the objective of a conditional GAN and
L𝐿1 (𝐺) is the L1 loss encouraging pixel-by-pixel accuracy and less
blurring. 𝑥 is the input images in source domain, 𝑦 is the label
images in target domain, and 𝑧 is a random vector sampled from
normal distribution.

Here, we propose a novel GAN-based framework to solve the
domain transfer tasks for urban images, namely MetroGAN. As
shown in Figure 1, we adopt L1 loss and the U-Net generator in

pix2pixGAN [16] to reinforce the information of corresponding lo-
cations in input and output images. To incorporate the complicated
hierarchical features of the urban system, we propose a progres-
sive growing training procedure in our model. We also design a
geographical constraint loss to impose a soft constraint that cities
cannot be on the water. Besides, to solve the instability problem
in the training process, we transform the adversarial loss to mini-
mize the Pearson 𝜒 2 divergence following LSGAN [26]. Our final
objective is

𝐺 ∗ = arg min
𝐺

max
𝐷

L𝐿𝑆𝐺𝐴𝑁 (𝐺, 𝐷)+

𝜆𝐿1L𝐿1 (𝐺) + 𝜆𝑔𝑒𝑜 L𝑔𝑒𝑜 (𝐺),

(4)

where L𝐿𝑆𝐺𝐴𝑁 (𝐺, 𝐷) is the least square adversarial loss [26] replac-
ing L𝑐𝐺𝐴𝑁 (𝐺, 𝐷) (see Section 3.4), and L𝑔𝑒𝑜 (𝐺) is the geographical
constraint loss of water area (see Section 3.3). 𝜆𝐿1 and 𝜆𝑔𝑒𝑜 are the
scale factors of L1 loss and geographical constraint loss, which are
set as 50, 100 in our experiments.

3.3 Geographical Constraint of Water Body
Although the deep generative model can naturally learn the im-
pact of the physical geography, physics-informed machine learning
enlightens that we can add some geographical constraints to the
loss function, enforcing corresponding learning biases to the model
[18]. With more learning biases, the model can learn the mappings
in line with geographical knowledge and own better performance.
The geographical constraint that the buildings cannot be located
on water can be easily implemented. We use the Hadamard product
of the water area and the generated city image as the loss of the
water constraint. The water area image is a binary image, in which
1 represents water and 0 represents land. The output of MetroGAN
is a one-channel image, in which the value of the pixel represents
the probability of being an urban area. Their Hadamard product
can filter the pixels that generate urban area on the water, and the
larger the probability of being an urban area, the larger the loss is.
The geographical constraint loss can be expressed as follows:

L𝑤𝑎𝑡𝑒𝑟 (𝐺) = −E𝑥,𝑧 [𝑥𝑤𝑎𝑡𝑒𝑟 ⊙ 𝐺 (𝑥, 𝑧)]

(5)

where 𝑥𝑤𝑎𝑡𝑒𝑟 is the water area image.

3.4 Stabilizing the Training Process
One challenge in the study of generative adversarial networks is
the instability of its training [27]. This problem is very serious in
the setting of urban morphology simulation, which is possibly due
to the fact that the built-up area is a binary image, and the dark
pixels (value = 0) account for a large proportion in most cities.

Widely applied methods that stabilize the training process in-
clude Wasserstein GAN with Gradient Penalty (WGAN-GP) [14],
SpectralNorm [27], and Least Squares GAN (LS-GAN) [26]. WGAN-
GP is an improved version of original Wasserstein GAN [3]. It
transforms the objective function from KL divergence to Wasser-
stein distance and imposes a gradient penalty to softly satisfy the
Lipschitz constraint that the Wasserstein distance required. Spec-
tralNorm imposes weight normalization using spectral coefficient,
and LS-GAN transforms the objective function to minimize the
Pearson 𝜒 2 divergence.

KDD ’22, August 14–18, 2022, Washington, DC, USA.

Weiyu Zhang et al.

Figure 1: Architecture of MetroGAN

Our results show that WGAN-GP does not work well in image
translation tasks, while SpectralNorm and LS-GAN are both effec-
tive. For a detailed comparison, the results of the two methods are
listed in Section 5.6. We found that LS-GAN can not only stabilize
the training, but also improve the performance of the model. Thus,
the least square loss is finally chosen as our adversarial loss. The
objective of LSGAN is:

L𝐿𝑆𝐺𝐴𝑁 (𝐺, 𝐷) =

E𝑥,𝑦 (cid:2)(𝐷 (𝑥, 𝑦) − 1)2(cid:3) +

E𝑥,𝑧 (cid:2)(𝐷 (𝑥, 𝐺 (𝑥, 𝑧)))2(cid:3)

1
2
1
2

(6)

3.5 Progressive Growing Structure
To further improve the model, we introduce the progressive train-
ing structure, which has been validated to stabilize the model and
produce high-quality outputs [19]. The progressive training pro-
cedure starts from low-resolution images and then gradually adds
layers to deal with high-resolution images. Correspondingly, an
urban system is a highly self-organized system with self-similarity
and hierarchical structure, and with progressive training, our model
can incorporate these characteristics better.

Based on the thought of progressive growth, we design our
generator (U-Net) and discriminator to add layers progressively in
the learning process. For the generator, we split it as the encoder
block and the progressive growing decoder block. The structure
of the encoder block is fixed, and the decoder block grows from
producing 4×4 images to producing 256×256 images. Similar to
the decoder block, the discriminator’s layers are also progressively
added in synchrony with the decoder, from taking 4×4 images
as input to taking 256×256 images. For resolution transition, we
design a mechanism based on [19] to fade the new layers in and
preserve well-trained last-level parameters. After adding new layers

to the decoder block, we weighted sum the output of the new layers
with weight 𝛼 and upsampled outputs of the last layer with weight
(1 − 𝛼) as the output. The weight 𝛼 gradually increases to 1 to fade
the new layer in smoothly. The procedure of the discriminator is
a mirror with the decoder, summing the output of the new layer
and downsampled results of input as the new input of D. Section
5.5 describes the improvement brought by the progressive training
methodology.

4 EVALUATION FRAMEWORK
To evaluate our results and compare the similarity between the gen-
erated city and the labeled city, we need to consider two questions:

(1) How much does the generated city image look like the la-

beled city?

(2) Does the generated city have similar complex features as

real cities?

In response to the first question, we compare our model with
existing models at three levels: the pixel level, the multi-scale spa-
tial level, and the perceptual level, focusing on the quantitative
differences between the cities generated by different models and
the labeled cities. For the second question, complexity science sheds
some light. As a typical complex system, cities follow some macro-
scopic laws, such as the Zipf’s law [17, 25] and the fractal structure
[5]. Thus, we use the fractal dimension and Zipf’s exponent to
evaluate the generated cities at a macroscopic level. Next, we detail
the evaluation metrics under different levels (Table 1).

4.1 Pixel Level
At the pixel level, we use MSE (mean square error) and PSNR (peak
signal to noise ratio) as the pixel-by-pixel accuracy evaluation

MetroGAN: Simulating Urban Morphology with Generative Adversarial Network

KDD ’22, August 14–18, 2022, Washington, DC, USA.

Table 1: Summary of the evaluation framework

Table 2: Summary of data sources

Purpose

Level

Comparison

Pixel Level
multi-scale spatial level
Perceptual Level

Metrics

PSNR
SPM
SSIM / LPIPS

Validation

Macroscopic Level

Fractal Dimension /
Zipf’s Law

metrics.

𝑀𝑆𝐸 =

1
𝑚𝑛

𝑚−1
∑︁

𝑛−1
∑︁

𝑖=0

𝑗=0

∥𝐼 (𝑥, 𝑦) − 𝐾 (𝑥, 𝑦)∥2

𝑃𝑆𝑁 𝑅 = 10 log10 (

𝑀𝐴𝑋 2
𝐼
𝑀𝑆𝐸

)

(7)

(8)

4.2 Multi-Scale Spatial Level
Previous studies dealing with spatial data usually represent the
location information using the hierarchical-resolution gridding
[12, 23], (e.g., locate the objects in grid segmentation of 4×4, 8×8,
. . . ). Following this idea, we improve the spatial pyramid matching
(SPM) metric [20] at this level. SPM is a solution to evaluate the
similarity of the distribution of feature points considering spatial
information, and we assume that every urban area pixel is a feature
point to calculate the SPM score. The procedure of calculating the
SPM score can be divided into the following steps:

First, for images X and Y, divide them with different scales (in 𝑙
layer divide the side length into 2𝐿). Under the L-layer segmentation,
the feature points in the corresponding sub-region of two images
are counted for all subregions. The smaller count is taken as the
value of each subregion. Then sum all subregions as the layer’s
score I (𝐻𝑙

𝑋 , 𝐻𝑙

𝑌 ).

I (𝐻𝑙

𝑋 , 𝐻𝑙

𝑌 ) =

𝐷
∑︁

𝑖=1

𝑚𝑖𝑛(𝐻𝑙

𝑋 (𝑖), 𝐻𝑙

𝑌 (𝑖))

(9)

The scores for each layer are summed as follows (The finer scale,

the larger weights):

K𝐿 (𝑋, 𝑌 ) = I𝐿 +

𝐿−1
∑︁

1

2𝐿−𝑙 (I𝑙 − I𝑙+1)

(10)

𝑙=1
To punish the behavior of generating much larger area than the
label to cover as many label pixels as possible, we regularize the
score of SPM by the max count of bright pixels in output and label
images. Besides, because the magnitude of regularized SPM score
is so small, we multiply it by 100.

Type

Source

Spatial
Resolution

Temporal
Resolution

Temopral
Range

Administrative
Area

GADM

None

Built-up Area

GHSL

Population

LandScan

DEM

Water Area

SRTM

GSW

30m

1km

30m

30m

Nighttime light

DMSP-OLS

1km

NPP-VIIRS

500m

None

15yr

1yr

None

None

1yr

1yr

None

1975-2014

2000-2020

None

None

1992-2014

2014-2020

4.4 Macroscopic Level
The fractal feature is one of the most important morphological
characteristics of the city and the fractal dimension is proposed to
measure this feature and the degree of self-similarity. Here, we use
the box counting [32] method to calculate the fractal dimension,
which is defined as

𝑑𝑖𝑚𝑏𝑜𝑥 (𝑆) := lim
𝜖→0

𝑙𝑜𝑔𝑁 (𝜖)
𝑙𝑜𝑔(1/𝜖)

,

(12)

where 𝑁 (𝜖)is the count of boxes needed to cover the city boundaries
using the box with side length 𝜖.

Unlike the fractal dimension, Zipf’s law is a statistical law that
describes the size distribution of the settlements in the urban system.
According to Zipf’s law, the relationship between the size of the
settlements and their frequencies of as follows:

𝑃 (𝑆) = 𝑘𝑆𝛾 ,

(13)

where 𝛾 is approximately -2. [17].

5 EXPERIMENTS
5.1 Data
To perform our experiments, we construct and publish a dataset of
global cities. Table 2 presents the types, sources, spatial resolution,
temporal resolution, and temporal range of our dataset. The GHSL
built-up area data [33], published in 1975, 1990, 2000 and 2014,
are binary images that urban area pixels have a value of 1 and
non-urban area pixels are 0. Each pixel of the DEM data [28] is
the average elevation of the corresponding area. The water area

𝑆𝑃𝑀 =

1
max(𝑁 (𝑙𝑎𝑏𝑒𝑙), 𝑁 (𝑜𝑢𝑡𝑝𝑢𝑡))

K𝐿 (𝑋, 𝑌 )

(11)

4.3 Perceptual Level
At the perceptual level, we use SSIM (Structural Similarity Index)
and LPIPS ( Learned Perceptual Image Patch Similarity) [35] metrics
to extract images’ structural features. SSIM can only extract shallow
attributes, while LPIPS, with the deep neural network, can extract
deep structural features of an image.

Figure 2: Input samples. The built-up area and water area
are binary images, in which value 1 means urban/water area.
The value of DEM and NTL are scaled to [0,1].

KDD ’22, August 14–18, 2022, Washington, DC, USA.

Weiyu Zhang et al.

data [29] is also a binary map that the water area pixels have a
value of 1, and others are set to 0. We use two types of nightlight
datasets: DMSP-OLS [11] and NPP-VIIRS [10]. NPP-VIIRS NTL data
has improved the problems of blurring and saturation of the DMSP-
OLS data to a great extent. However, its temporal range is relatively
short and cannot be used to extract historical urban areas before
2012.

We select global cities with more than 10,000 populations (see
Appendix for details). Then we clip city images representing 100km
× 100 km spatial extent and resize the images to 128×128 (for DMSP-
OLS data) and to 256×256 (for NPP-VIIRS data). After filtering the
built-up area images with less than 1% amount of signal (bright
pixels account for less than 1%), only approximately 6,000 cities
were left. To solve the problem of insufficient training data, we
combine the data of 2000 and 2014 and obtain a dataset with 11,873
cities. Notably, the NPP-VIIRS NTL data are published from 2012
to 2021, and only one snapshot of the built-up area data (2014) is
within this period. We therefore construct the 256×256 dataset with
single-year data and 128×128 dataset with multi-year data.

5.2 Baselines
To show the effectiveness of our model, we compare MetroGAN
with the following baselines:
• XGBoost [9] is a scalable tree boosting system based on gradient
boosting tree model. Methods based on classification have been
widely used in geography to simulate urban area and the XGBoost
classification model is a state-of-the-art method of them [22] .
We input each pixel’s information (NTL, DEM and water area)
to the model to classify a pixel as urban area or not.

• U-Net [31] is a popular deep learning method for image trans-
lation tasks, but has not been applied in urban simulation tasks.
Using U-Net, we apply DEM, water, and NTL as inputs to gener-
ate cities.

• CityGAN [1] (we use the name of related GitHub repository for
the model) is the state-of-the-art GAN-based model for the urban
morphology simulation task. This model is based on pix2pix GAN
and takes DEM, water, and NTL as inputs in our experiments.

5.3 Comparison with Baselines
Some representative images of the results are shown in Figure 3 and
the results of MetroGAN and baselines are summarized in Table 3.
To visually show the comparison, we also present the ground truth
and the generated urban area of different models in Figure 4.

Table 3 shows that MetroGAN outperforms all other baselines
in both 128×128 dataset and 256×256 dataset. Due to the blur of
nightlight data, it is more challenging to produce cities using the
128×128 dataset. In this dataset, our model achieves 25.3% and
21.1% improvements over the best-performing baseline in terms of
SPM and LPIPS, respectively, indicating that our model is good at
capturing the spatial and deep structured information of the urban
system. Our model also improves 7.1% performance in PSNR and
6.5% in SSIM compared with U-Net. Compared with CityGAN, the
improvements are up to 20.39% and 19.8%. This reveals that our
model comprehensively improves the performance in all aspects,
especially when facing blurring inputs. Interestingly, in the 256×
256 dataset, the performance in PSNR and SSIM of the four methods

are much closer than in the 128×128 dataset (5.6% in PSNR and 0.0%
in SSIM), but our model can still outperform the best baseline in
terms of SPM and LPIPS (13.8% in SPM and 5.2% in LPIPS). This
implies that with the increase of the spatial resolution of NTL maps,
all models own a better performance on pixel-by-pixel accuracy,
but generating the overall spatial structures and leveraging the
information of terrains and water body is still hard for baselines,
especially XGBoost.

Table 3: Performance comparison

Pixel Level

Multi-scale
Spatial Level

Perceptual Level

PSNR↑

SPM↑

SSIM↑

LPIPS↓

128×128 Dataset

XGBoost
U-NET
CityGAN
MetroGAN

9.3806
11.7633
10.4691
12.6011

256×256 Dataset

XGBoost
U-NET
CityGAN
MetroGAN

15.6972
15.5728
15.4984
16.5702

0.1577
0.1772
0.1669
0.2232

0.2122
0.2201
0.2239
0.2547

0.4659
0.4771
0.4241
0.5083

0.5839
0.6201
0.5408
0.6203

0.6593
0.3405
0.3408
0.2687

0.4463
0.4110
0.4053
0.3842

5.4 Validation of Macrocopic Level Metrics
From the macroscopic level, we validate whether the generated
images are like a real city by calculating the Zipf’s exponent and
fractal dimensions. Specifically, we calculated the fractal dimension
of all generated cities (x-axis of Figure 5) and label cities (y-axis
of Figure 5) in the test set. The Pearson correlation coefficient be-
tween fractal dimensions of the label cities and the generated cities
is 0.823 and the MAPE is 0.052. These results reveal that the fractal
dimensions of generated cities are very close to the label cities, in-
dicating that our model perfectly captures the cities’ characteristics
of fractality. For those outliers, we find that most of them are cities
whose built-up images contain few pixels (One typical outlier is
shown in Figure 5 and others are shown in appendices). The small
amount of urban area might mean the urban system is not mature,
and the calculation of fractal dimension has low sensitivity; that is,
few wrongly classified bright pixels can affect the fractal dimension
to a large extent.

Then, we calculate the Zipf’s exponent by log-transforming Eq.
13 to 𝑙𝑛(𝑃 (𝐴)) = 𝛾𝑙𝑛(𝐴) + 𝐶. The exponent 𝛾 is the coefficient
of the linear regression model. Figure 6 shows that the 𝛾s of our
generated cities are around -2, indicating a well-fitted Zipf’s law.
Specifically, in the 256×256 dataset, the exponents fall within [-
1.75,-2.25] in 76/100 cities and within [-1.5,-2.5] in 97/100 cities. In
128×128 dataset, the coefficients fall within [-1.75,-2.25] in 84/100
cities and within [-1.5,-2.5] in 98/100 cities.

MetroGAN: Simulating Urban Morphology with Generative Adversarial Network

KDD ’22, August 14–18, 2022, Washington, DC, USA.

(a) Results of 128×128 Dataset

(b) Results of 256×256 Dataset

(c) Results of physical geography inputs

Figure 3: Randomly sampled results of three experiments

Figure 5: Results of fractal dimension

on key questions, such as to what extent the physical geography
can influence the urban form or what kind of physical geographic
variables influence the urban form most. Some samples of the results
are shown in Figure 3c.

The results show that in some special natural conditions, such
as the city being along a valley, in a basin, or by the bay, our model
can generate a rough shape and some textures of the city. This
implies that when the blooming of the cities is restricted by spatial
conditions, the impact of nature is greatly enhanced. Conversely, if
a city is in a vast plain and has no spatial constraint, the impact of
nature is relatively low, which is revealed in our results that cities
grow isotropically. In these cases, other types of urban data, such
as the location of the first settlement and the distribution of natural
resources, may be useful for further explorations.

5.6 Ablation Study
To comprehensively analyze the effectiveness of the various com-
ponents in our model, we conduct two ablation experiments.

Figure 4: Comparison of results of MetroGAN and baselines

5.5 Generating Urban Built-up Area from

Physical Geographic Features

We also conducted an experiment to simulate urban areas just from
DEM and water using the 128×128 dataset. This experiment can be
of great use in the research of urban geography to shed some lights

KDD ’22, August 14–18, 2022, Washington, DC, USA.

Weiyu Zhang et al.

Table 4: Ablation analysis. The components that are used in MetroGAN and improve the performance are boldfaced.

Ablation 1

Ablation 2

PSNR↑

SPM↑

SSIM↑

LPIPS↓

PSNR↑

SPM↑

SSIM↑

LPIPS↓

Base1
WGP
SN
LS

10.4690
10.5210
11.7227
11.7631

0.1669
0.1671
0.1872
0.2115

0.3673
0.4080
0.4661
0.4792

0.3408
0.3441
0.3076
0.2843

Base2
-DV
+GC
+PT

11.7631
9.4430
12.2924
12.5146

0.2115
0.1701
0.2227
0.2219

0.4792
0.3112
0.5030
0.4938

0.2843
0.3129
0.2725
0.2610

masks to outputs of the base method, constraining cities are not on
the water. We consider the following variants:

• -DV (data volume): We remove the data of the year 2000 from the
128×128 dataset, getting a single-year dataset with 6384 cities.
• +GC (Geographical constraint): We add our geographical con-

straint loss in addition to the least square loss and L1 loss.

• +PT (Progressive Training): We design a progressive training
procedure for the decoder block of the U-Net generator and
discriminator (See Section 3.5).

The results of this experiment are listed in Table 4, Ablation2. Re-
sults show that the data volume can affect the performance to a
large extent, and this perhaps implies why the advantages of Met-
roGAN are narrowed in the 256×256 dataset. The geographical
constraint proves to be helpful, which leads to an improvement of
4.5%, 5.3%, 5.0%, and 4.1% with respect to the four metrics. This is
because with the geographical constraint loss, we directly tell the
model not to generate urban areas on the water, and the model just
needs to learn the influence of water areas on surrounding regions.
The progressive training procedure also brings 6.4%, 5.0%, 3%, and
8.9% improvements in terms of four metrics respectively, indicat-
ing that the progressive growing structure can stably improve the
model in all aspects.

6 CONCLUSIONS AND FUTURE WORK
In this research, we propose the MetroGAN model, which is in-
formed with geographical knowledge, to solve the instability prob-
lem of the previous GAN-based model for city simulation tasks, and
we validate MetroGAN’s ability to incorporate various urban at-
tributes (e.g., input physical geography singly). In addition, because
of cities’ highly structural and hierarchical characteristics, any sin-
gle metric is insufficient to evaluate generated cities. We propose a
comprehensive evaluation framework for assessing whether a gen-
erated city is like a real city and how similar the generated city and
the corresponding real city are, which makes up the shortcomings
of the previous city extraction evaluation standards. With the new
evaluation standards, we validated the superiority of our model on
three baselines. Through ablation study, we show the comparison of
mainstream GAN stabilizing techniques and the significant effect of
geographical loss, the volume of the urban dataset, and progressive
training in urban form simulation tasks. Finally, we found that with
physical geographic features (terrain and water area), our model
can still generate city morphology under certain natural conditions,
inspiring urban geography to figure out the relationship between
nature and the city.

Figure 6: Distribution of Zipf’s exponents for 128×128
dataset and 256×256 dataset

5.6.1 The structures to stabilize training. We test three methods
to stabilize the training process and improve the performance. In
Ablation 1, we use pix2pixGAN as our base model (Base1 in Table
4) and add the following three structures in the base model to test
their effectiveness.
• WGAN-GP (Wasserstein GAN with Gradient Penalty): We replace
the original loss with Wasserstein loss and add the penalty term
into the loss function.

• LS (Least Square Loss): We use the mean square error loss, re-
placing the original binary cross entropy loss, when calculating
the loss of the discriminator.

• SN (Spectral Normalization): We add spectral norm operation to

all convolutional layers in the discriminator.
The results are summarized in Table 4, Ablation1. From the
results, we can tell WGAN-GP has little or no improvement in
the base model. Meanwhile, we find that the non-convergence
phenomena of the original model still appear sometimes. Spectral
Normalization and Least Square Loss can eliminate the problem
of non-convergence and unstable training, and the results show
that using LS Loss can bring larger improvement than Spectral
Normalization. In addition, it is worth mentioning that adding both
LS and SN does not work because both of them will constrain the
gradients to too small.

5.6.2 The components to improve the performance. To validate the
other components in our model, we did another ablation experiment
using the LSGAN trained on multi-year 128×128 dataset as the base
method. It should be mentioned that we also use the water maps as

MetroGAN: Simulating Urban Morphology with Generative Adversarial Network

KDD ’22, August 14–18, 2022, Washington, DC, USA.

In future works, we plan to extend our model both spatially and
temporally. From spatial perspectives, we can utilize more complex
inputs into the model, such as Industrial structures and their spatial
configurations, and consider urban systems with a larger spatial
scale. Temporally, we plan to add recurrent neural networks into
our model like [30] to simulate the evolution process of urban
systems. Futhermore, we intend to apply interpretation methods
to MetroGAN to figure out how urban attributes influence the city,
and data augmentation approaches will also be added to further
solve the problem of data shortage.

ACKNOWLEDGMENTS
This research was funded by the National Natural Science Foun-
dation of China (41830645, 41971331). Dr. Di Zhu is supported by
the Faculty Set-up Funding of College of Liberal Arts, University of
Minnesota (1000-10964-20042-5672018).

REFERENCES
[1] Adrian Albert, Jasleen Kaur, Emanuele Strano, and Marta Gonzalez. 2019. Spa-
tial sensitivity analysis for urban land use prediction with physics-constrained
conditional generative adversarial networks. arXiv:1907.09543

[2] Adrian Albert, Emanuele Strano, Jasleen Kaur, and Marta González. 2018. Model-
ing urbanization patterns with generative adversarial networks. In IGARSS 2018 -
2018 IEEE International Geoscience and Remote Sensing Symposium. 2095–2098.

[3] Martin Arjovsky, Soumith Chintala, and Léon Bottou. 2017. Wasserstein gen-
erative adversarial networks. In International Conference on Machine Learning.
PMLR, 214–223.

[4] Michael Batty. 2008. The size, scale, and shape of cities. Science 319, 5864 (2008),

769–771.

[5] Michael Batty and Paul A Longley. 1994. Fractal Cities: A Geometry of Form and

Function. Academic Press.

[6] Luís M. A. Bettencourt, José Lobo, Dirk Helbing, Christian Kühnert, and Geof-
frey B. West. 2007. Growth, innovation, scaling, and the pace of life in cities.
Proceedings of the National Academy of Sciences 104, 17 (2007), 7301–7306.
[7] Maarten Bosker and Eltjo Buringh. 2017. City seeds: geography and the origins
of the european city system. Journal of Urban Economics 98 (2017), 139–157.
[8] Wenpu Cao, Lei Dong, Lun Wu, and Yu Liu. 2020. Quantifying urban areas with
multi-source data based on percolation theory. Remote Sensing of Environment
241 (2020), 111730.

[9] Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A scalable tree boosting
system. In Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (San Francisco, California, USA) (KDD
’16). Association for Computing Machinery, New York, NY, USA, 785–794.
[10] Christopher D Elvidge, Kimberly Baugh, Mikhail Zhizhin, Feng Chi Hsu, and
Tilottama Ghosh. 2017. VIIRS night-time lights. International Journal of Remote
Sensing 38, 21 (2017), 5860–5879.

[11] Christopher D Elvidge, Kimberly E Baugh, Eric A Kihn, Herbert W Kroehl, and
Ethan R Davis. 1997. Mapping city lights with nighttime data from the DMSP
Operational Linescan System. Photogrammetric Engineering and Remote Sensing
63, 6 (1997), 727–734.

[12] Tao-yang Fu and Wang-Chien Lee. 2021. ProgRPGAN: Progressive GAN for
route planning. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining. Association for Computing Machinery, New York, NY,
USA, 393–403.

[13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversar-
ial nets. In Advances in Neural Information Processing Systems, Vol. 27. Curran
Associates, Inc.

[14] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and
Aaron C Courville. 2017. Improved training of wasserstein GANs. In Advances in
Neural Information Processing Systems, Vol. 30. Curran Associates, Inc.

[15] Xiaoman Huang, Annemarie Schneider, and Mark A Friedl. 2016. Mapping sub-
pixel urban expansion in China using MODIS and DMSP/OLS nighttime lights.

Remote Sensing of Environment 175 (2016), 92–108.

[16] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. 2017. Image-to-
image translation with conditional adversarial networks. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

[17] Bin Jiang and Tao Jia. 2011. Zipf’s law for all the natural cities in the United
States: A geospatial perspective. International Journal of Geographical Information
Science 25, 8 (2011), 1269–1281.

[18] George Em Karniadakis, Ioannis G. Kevrekidis, Lu Lu, Paris Perdikaris, Sifan
Wang, and Liu Yang. 2021. Physics-informed machine learning. Nature Reviews
Physics 3, 6 (2021), 422–440.

[19] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2017. Progressive
growing of GANs for improved quality, stability, and variation. arXiv:1710.10196
[20] Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. 2006. Beyond bags of
features: Spatial pyramid matching for recognizing natural scene categories. 2006
IEEE Computer Society Conference on Computer Vision and Pattern Recognition
(CVPR’06) 2 (2006), 2169–2178.

[21] Ruiqi Li, Lei Dong, Jiang Zhang, Xinran Wang, Wen-Xu Wang, Zengru Di, and
H. Eugene Stanley. 2017. Simple spatial scaling rules behind complex cities.
Nature Communications 8, 1 (2017), 1841.

[22] Xuecao Li and Yuyu Zhou. 2017. Urban mapping using DMSP/OLS stable night-
time light: A review. International Journal of Remote Sensing 38, 21 (2017), 6030–
6046.

[23] Defu Lian, Yongji Wu, Yong Ge, Xing Xie, and Enhong Chen. 2020. Geography-
aware sequential location recommendation. In Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining. Associ-
ation for Computing Machinery, New York, NY, USA, 2009–2019.

[24] Xiaoping Liu, Lei Ma, Xia Li, Bin Ai, Shaoying Li, and Zhijian He. 2014. Simulat-
ing urban growth by integrating landscape expansion index (LEI) and cellular
automata. International Journal of Geographical Information Science 28, 1 (2014),
148–163.

[25] Hernán A. Makse, Shlomo Havlin, and H. Eugene Stanley. 1995. Modelling urban

growth patterns. Nature 377, 6550 (1995), 608–612.

[26] Xudong Mao, Qing Li, Haoran Xie, Raymond Y. K. Lau, Zhen Wang, and
Stephen Paul Smolley. 2017. Least squares generative adversarial networks.
In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy,
October 22-29, 2017. IEEE Computer Society, 2813–2821.

[27] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. 2018.
Spectral normalization for generative adversarial networks. In International Con-
ference on Learning Representations.

[28] Earth Resources Observation and Science (EROS) Center. 2017. Shuttle radar

topography mission (SRTM) 1 arc-second global.

[29] Jean-François Pekel, Andrew Cottam, Noel Gorelick, and Alan S. Belward. 2016.
High-resolution mapping of global surface water and its long-term changes.
Nature 540, 7633 (2016), 418–422.

[30] Suman Ravuri, Karel Lenc, Matthew Willson, Dmitry Kangin, Remi Lam, Pi-
otr Mirowski, Megan Fitzsimons, Maria Athanassiadou, Sheleem Kashem, Sam
Madge, Rachel Prudden, Amol Mandhane, Aidan Clark, Andrew Brock, Karen
Simonyan, Raia Hadsell, Niall Robinson, Ellen Clancy, Alberto Arribas, and Shakir
Mohamed. 2021. Skilful precipitation nowcasting using deep generative models
of radar. Nature 597, 7878 (2021), 672–677.

[31] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-Net: Convolutional
networks for biomedical image segmentation. In Medical Image Computing and
Computer-Assisted Intervention (MICCAI), Vol. 9351. Springer, Cham, 234–241.

[32] Nirupam Sarkar and Bidyut Baran Chaudhuri. 1994. An efficient differential
box-counting approach to compute fractal dimension of image. IEEE Transactions
on systems, man, and cybernetics 24, 1 (1994), 115–120.

[33] Marcello Schiavina, Michele Melchiorri, and Sergio Freire. 2021. GHS_DUC
R2019A - GHS Degree of Urbanisation Classification (2015, 2000, 1990, 1975),
R2019A. (2021).

[34] Fengli Xu, Yong Li, Depeng Jin, Jianhua Lu, and Chaoming Song. 2021. Emergence
of urban growth patterns from human mobility behavior. Nature Computational
Science 1, 12 (2021), 791–800.

[35] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang.
2018. The unreasonable effectiveness of deep features as a perceptual metric. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR).

[36] Di Zhu, Ximeng Cheng, Fan Zhang, Xin Yao, Yong Gao, and Yu Liu. 2020. Spatial
interpolation using conditional generative adversarial neural networks. Interna-
tional Journal of Geographical Information Science 34, 4 (2020), 735–758.

KDD ’22, August 14–18, 2022, Washington, DC, USA.

Weiyu Zhang et al.

A IMPLEMENTATION DETAILS
A.1 Dataset Details
There are a few datasets for urban morphology simulation, while
all of them are inaccessible, and none of them includes terrains
maps. Therefore, we build and publish a new dataset for the urban
morphology simulation task, containing DEM, NTL, water area,
and built-up area data. These data are collected from seven data
sources (See Table 2). The procedures of building the dataset are as
follows:

(1) We calculate the total population of level 2 administrative di-
visions (e.g., prefecture-level cities in China) with the GADM
administrative area dataset and the LandScan population den-
sity map.

(2) We select the cities whose people are more than 10,000 and
choose the center point of the cities by finding the pixel with
maximum population density in the corresponding administra-
tive district.

(3) For every selected city, we clip an image using a rectangle win-
dow of 100km width and height (100km geographical distance
corresponds to different pixels in different latitudes) around the
center point.

(4) Because the DMSP-OLS NTL data and the NPP-VIIRS NTL data
have different spatial resolutions, we build two datasets for
them. We resize images in the dataset of DMSP-OLS to 128×128,
and the images in the dataset of NPP-VIIRS to 256×256 (for
NPP-VIIRS).

(5) Then, we filter the images whose amount of signal is less than
1% (bright pixels in the built-up area account for less than 1%).
We randomly chose 200 cities as the test set.

(6) Since after filtering, only about 6000 cities are left in a single-
year dataset, which is not enough for GAN’s training, we collect
2000 and 2014 data to build 128×128 dataset (The built-up area
dataset was published in 1975,1990,2000,2014), obtaining 11773
cities. But in the 256 × 256 dataset, the NPP-VIIRS dataset was
published from 2012, and only the 2014 built-up area dataset is
in the period. And thus, we have to build the 256×256 dataset
with only 2014 data.

A.2 Model Details

Architecture Selection: In the GAN-based models training pro-
cesses, the discriminator is very easy to overfit because classifying
tasks of D is easier to learn than generating tasks of G. To address
this problem, we set the architecture of the discriminator much
simpler than the generator.

Progressive Growing Training Procedure: We add a progressive
growing structure to the decoder block of the U-Net generator and
discriminator. The structure of the encoder block of G is static, and
the new layers of decoder block and discriminator fade in. We set
the weight of the new layer 𝛼 starts from 0 and gradually grows to
1.

Moreover, the StyleGAN points out that only the growing G is
enough, and when G is growing, the pixel-by-pixel loss is used, and
other losses are not used until G grows to the highest layers. We
tested this thought and found that using only the growing decoder
and just L1 loss when it is growing does not decline the performance

(a) Distribution of global cities

(b) Distribution of built-up area

(c) Distribution of DEM

(d) Distribution of nighttime light

Figure 7: Global distribution of the data sources

of MetroGAN. The adversarial loss and geographical loss are added
after the G is grown to the highest level.

A.3 Experimental Settings and Training Details
We have two experimental settings. One is using NTL, DEM, and
water area data as three inputs. The three inputs are all distribution
maps of corresponding attributes, and the NTL data can specify
the stage of the city. The other setting is preserving the physical
geography inputs (DEM and water area) but without NTL data. In
this setting, we need to input additional city property indicators
to indicate the stage of cities. Otherwise, in theory, the model can

MetroGAN: Simulating Urban Morphology with Generative Adversarial Network

KDD ’22, August 14–18, 2022, Washington, DC, USA.

image whose pixels’ value ranges in [0,1], and we cut off the output
with a threshold of 0.9.

All of our experiments are conducted on a server with two
NVIDIA TITAN RTX GPUs with 24GB RAM and one Intel(R) Xeon(R)
Silver 4210 CPU @ 2.20GHz on Ubuntu 18.04. The MACs of U-Net
and generator in CityGAN and MetroGAN are 13.34G. The MACs
of discriminator in CityGAN and MetroGAN are 1.28G. The MACs
of these models are comparable, and MetroGAN shows its superi-
ority with better performances. The source code and the dataset
are available at Github (https://github.com/zwy-Giser/MetroGAN).

B ADDITIONAL ANALYSIS OF RESULTS
B.1 Analysis of results with physical

geography inputs

In the experiments with physical constraints input, we find that in
special conditions, the urban morphology can be simulated vividly
by our model. Specifically, we qualitatively summarize the condi-
tions into four types.
• Restricted by the sea. Most cities by the sea can be well modeled.
This is because one or two sides of the urban morphology have
been restricted by the coast. Besides, the urban center of coastal
cities is always along the coast, and the other side of the coast
may be restricted by terrains, such as mountains.

• Guided by the river. In some cases that contain rivers, the spines

of the urban morphology follow the flow of the river.

• Restricted by terrains. When the restrictions of terrains are strong,
such as in a basin, the model can generate more accurate urban
forms because available space is limited.

• Few Restrictions. When there are few restrictions on water and
terrains, such as on a plain, the urban growth is likely driven by
other factors. The most significant difference in these cases is that
the real cities reflect strong agglomeration effects, but simulated
cities do not. This is because our model does not consider the
spatial interaction process.

B.2 Outliers of Macroscopic Level Validation
This section shows some outliers of fractal dimension validation.
The outliers are picked with the standard that the difference be-
tween FD of the real city and FD of the generated city is larger
than 0.2. And we find that these outliers all have a small number of
bright pixels(less than 15% pixels). The small amount of urban area
means the calculation of these cities’ fractal dimensions has low
sensitivity. Only a few wrongly classified bright pixels can affect
the fractal dimension to a large extent.

Figure 9: Outliers of fractal dimension validation

(a) Type 1: Restricted by the sea

(b) Type 2: Guided by the river

(c) Type 3: Restricted by terrains

(d) Type4: Few restrictions

Figure 8: Classification of Results of Physical Geography

generate the urban morphology of any historical phase at this
location. Here, we use the total population of the cities and input it
as a single value image.

For hyperparameters, We tested the value of 𝜆𝐿1 and 𝜆𝑔𝑒𝑜 , and
find when their values are 50 and 100, we can get satisfactory results.
In all experiments, we set the batch size as 64, and we adopt Cosine
Annealing with Warm Retarts learning rate scheduler and Adam
optimizer(b1=0.5, b2=0.99). The number of iterations is 750K, and
the initial learning rate is 10−4. The output of the generator is an

