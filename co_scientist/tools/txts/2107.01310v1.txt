Clustering of Time Series Data with Prior
Geographical Information

Reza Asadi, and Amelia Regan

1

Abstract

Time Series data are broadly studied in various domains of transportation systems. Trafﬁc data are
a challenging example of spatio-temporal data, as it is multi-variate time series with high correlations in
spatial and temporal neighborhoods. Spatio-temporal clustering of trafﬁc ﬂow data ﬁnd similar patterns
in both spatial and temporal domain, where it provides better capability for analyzing a transportation
network, and improving related machine learning models, such as trafﬁc ﬂow prediction and anomaly
detection. In this paper, we propose a spatio-temporal clustering model, where it clusters time series
data based on spatial and temporal contexts. We propose a variation of a Deep Embedded Clustering
(DEC) model for ﬁnding spatio-temporal clusters. The proposed model Spatial-DEC (S-DEC) use prior
geographical information in building latent feature representations. We also deﬁne evaluation metrics for
spatio-temporal clusters. Not only do the obtained clusters have better temporal similarity when evaluated
using DTW distance, but also the clusters better represents spatial connectivity and dis-connectivity. We
use trafﬁc ﬂow data obtained by PeMS in our analysis. The results show that the proposed Spatial-DEC
can ﬁnd more desired spatio-temporal clusters.

Deep Learning, Time Series Clustering, Spatio-temporal data, Trafﬁc Flow Data

Index Terms

I. INTRODUCTION

Spatio-temporal data arise in broad areas of engineering and environmental sciences. Data mining
techniques have been used extensively for spatio-temporal analysis [1]. Geo-referenced time
series are a subset of spatio-temporal data, where ﬁxed locations over a geographical area
observes some features for a time period in a synchronous way. Trafﬁc data is a complex example
of Geo-referenced data, which is multi-variate time series data, including the ﬂow, speed and
occupancy of a large number of sensors, and in which there are correlations and similarities in
spatial and temporal neighborhood. Spatio-temporal analysis of trafﬁc data have a pivotal role
in future research to improve the performance of transportation systems [2], such as reducing
trafﬁc congestion and air pollution [3], understanding the behaviour of a transportation network
[4], predicting trafﬁc speed and ﬂow [5], [6], and detecting non-recurrent congestion events [7].

The volume and variety of spatio-temporal data has increased with the advent of new sensing
technologies, such as cameras, GPS and sensors [8]. Increases in the volume of trafﬁc data
requires the development of large-scale machine learning algorithms and big data analytics
[9], and data-driven approaches on trafﬁc data [10]. Deep learning models have been recently
successfully applied on spatial and temporal domains [11]. The models especially outperforms
traditional machine learning and statistical methods on large-scale data. Several studies shows the
success of deep learning solutions, such as trafﬁc ﬂow forecasting [12], missing data imputation

Reza Asadi and Amelia Regan are with department of Computer Science at University of California Irvine, USA, e-mail:

{rasadi, aregan}@uci.edu

1
2
0
2

l
u
J

3

]

G
L
.
s
c
[

1
v
0
1
3
1
0
.
7
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
2

[13] and spatio-temporal modelling of trafﬁc ﬂow data [14]. Success of deep learning models
in various domains along with the challenges of applying the deep learning models on spatio-
temporal trafﬁc data are the main motivations to further study the problem.

A. Clustering of trafﬁc data

Spatio-temporal clustering of trafﬁc data has been broadly studied with various goals. First,
congestion detection and prediction can assist travelers and trafﬁc management systems to
improve the efﬁciency of existing systems. Second, detecting similarity in trafﬁc patterns can help
machine learning models to ﬁnd similar regions in a transportation network. This can improve
missing data imputation and trafﬁc forecasting models, or can identify anomalies in the data.

In [15], they propose an improvement of fuzzy k-means clustering to classify trafﬁc states into
ﬁve groups ranging from mild to extreme trafﬁc. Also, in [16], a clustering of trafﬁc ﬂow data
is obtained based on congestion levels. They describe clusters in the temporal domain based on
levels of congestion. While these works cluster trafﬁc data based on trafﬁc congestion, they did
not consider spatial domains in their analysis. Moreover, in [17], they propose a method to better
understand how trafﬁc conditions are correlated in space-time. They cluster trafﬁc data based
on four congestion levels using an improved spatio-temporal Moran scatter-plot. These works
cluster trafﬁc data based on level of congestion. However, we consider clustering of trafﬁc data
based on similarities of patterns, which can be more generalizable to various machine learning
problems, such as trafﬁc ﬂow prediction and anomaly detection. In [7], they deﬁne a measure,
called the Link Journey Time, and they obtain spatio-temporal clusters of non-recurrent events.
Each spatio-temporal cluster is a non-recurrent detected event, where it represents neighboring
spatial and temporal features. Their model consider a similarity measure to obtain spatio-temporal
data. However, their model is only designed to ﬁnd non-recurrent events, and not generalized to
ﬁnd similar regions or temporal patterns.

In [18], they consider the problem of clustering of trafﬁc ﬂow data to obtain spatial and
temporal similar patterns. They propose spatio-temporal clustering of trafﬁc ﬂow by considering
topology of the network and similarity of time series data, where clusters are made by successive
connections of neighbors. This work considers prior assumptions about the data and the topology
of the network. A data-driven approach is expected to ﬁnd spatio-temporal clusters without any
prior assumptions, which would be more generalizable to different problems and scenarios [19].
In [20], similarities of urban trafﬁc ﬂow are explored with a discrete wavelet transform. In
[21], they proposed a fuzzy clustering method on trafﬁc ﬂow segments. Dynamic Time Warping
(DTW), as a temporal similarity function, is used to identify locations with temporal similarities.
They consider the problem of clustering of time series segments. In [22], they represent spatio-
temporal data as image-like representation. They propose a point-based and segment-based
clustering of speed to represents classes of trafﬁc congestion in spatial and temporal domain. A
segment-based clustering is a similar approach to our model, where we ﬁnd the clustering based
on time series segments. However, they use a ﬁlter to obtain features from computer vision. On
the other hand, we consider a temporal similarity distance to represent similarities in trafﬁc ﬂow
data. Moreover, they evaluate clusters by visually assessing the model’s output, where we use
such a visualization method to represent interesting insight in the clusters of trafﬁc ﬂow data.

Similarity of trafﬁc patterns not only detect trafﬁc congestion, but also detect spatial and tem-
poral heterogeneous neighborhoods. In [23], a k-means clustering is applied to ﬁnd trafﬁc ﬂow
variations based on spatio-temporal correlations. The clusters of similar locations are the input to
a neural network which predicts trafﬁc ﬂow with higher performance. In [24], cluster of similar

3

locations have been used with an autoencoder to impute missing values. In [25], a clustering
method ﬁnds road segments based on their features and missing data imputation is applied on
incomplete speed data. In [26], a clustering model is used to identify anomalies in trafﬁc ﬂow
data. We consider the problem of discovering spatio-temporal similarities in trafﬁc data. Clusters
of trafﬁc data, such as speed, ﬂow and occupancy can represent levels of congestion. However,
trafﬁc ﬂow data can be represents locations and time stamps with similar patterns with the goal
of ﬁnding heterogeneous spatial and temporal domains.

B. Spatio-temporal clustering with deep learning

Since we consider clustering of trafﬁc ﬂow segments, here we review some of the recent research
regarding time series clustering and in the rest, we describe the literature review for deep learning
models for clustering problems. In [27], they describe a broad range of time series clustering
applications. The main components of time series clustering are studied including time series
representations, similarity and distance measures, clustering prototypes and time series clustering.
In [28], they describe the challenges of k-means clustering with time warp measures. They
propose weighted and kernel time warp measures for k-means clustering. Their method has a
faster estimation of clusters. Further investigation of time series clustering is studied in [29].
These works illustrate that novelty in time series representations and distance measures are the
main approaches of improving temporal clustering.

There are a broad range of clustering models applied to spatio-temporal data, such as k-means
[30], DBSCAN [31], agglomerative clustering [32], and matrix factorization based clustering
[33]. However, increases in the size of datasets requires more scalable models such as deep
learning models. When there is a huge dataset that includes data points with spatial and temporal
properties, applying traditional clustering methods such as k-means on trafﬁc data is computation-
ally expensive and can have poor performance [30]. More efﬁcient heuristic methods for k-means
clustering of trafﬁc ﬂow data have been studied [34]. Complex spatio-temporal patterns in trafﬁc
data necessitate further consideration of spatial and temporal information in the models. Deep
learning models signiﬁcantly improve performance of various machine learning problems, such
as computer vision and natural language processing. Deep learning models have been broadly
used for various large-scale spatio-temporal problems [11], [35]. Moreover, deep learning models
for clustering tasks are broadly studied in [36]. Deep embedded clustering is primarily introduced
in [37]. Variations of the model have been studied in broad domains. Joint training of the model
to preserve the latent feature space structure is proposed in [38]. In [39], they analyze clustering-
friendly latent representations, which jointly optimize dimension reduction using both a neural
network and k-means clustering. While most of the research applies deep embedded clustering on
images, there are few studies to show their performance on time series data. In [40], they jointly
cluster and train the model. They also segment time series data with agglomerative clustering.
In [41], they propose a DEC with a cluster tree structure to dynamically obtain the number of
clusters, while the original DEC has a ﬁx number of clusters. However, these models do not
consider any prior relation among the clusters. In [42], they propose a variation of DEC, which
considers the pairwise distance between data points. The model uses the prior distances as a
measure to classify unlabeled data points. This work consider a relation among the clusters for
unsupervised learning, and it is similar to our work, as we consider any prior relation among
the clusters based on their geographical information. In [43], they evaluate various similarity
metrics to obtain clusters with DEC. While these models consider temporal similarity in a DEC,
there is a lack of development of a deep learning model where it not only ﬁnds clusters based
on temporal similarity, but also prior spatial features would be considered.

4

C. Contributions of the work

The aforementioned works on clustering of time series data show the importance, advantages
and applications of applying deep learning models to cluster time series and spatio-temporal
trafﬁc data. It also shows that recently there have been several deep learning models developed
for clustering problems, where their goal is to modify latent feature space. There are various
works that develop deep learning models for clustering of time series data. Clustering of time
series data ﬁnds cluster of a transportation network based on the similarity of trafﬁc ﬂow data
[44]. However, considering prior geographical information in designing clusters is a challenging
problem.

In this paper, we focus on clustering of time series with prior geographical information. We
propose a model where it modify latent feature representation based on geographical information.
The model is a variation of DEC, where it ﬁnds spatio-temporal clusters by adding a new loss
function to the model.

The contributions of the paper are as follows:

• We formulate spatio-temporal clustering of trafﬁc ﬂow data as the clustering of time series

segments.

• A spatial deep embedded clustering (Spatial-DEC) model is proposed which considers
prior geographical information within the latent feature representation. To the best of our
knowledge, this is the ﬁrst work which considers prior geographical information to obtain
spatial clusters with the DEC model.

• We illustrate the application clustering of trafﬁc ﬂow data in transportation systems.
• The spatio-temporal clusters obtained by deep learning models are evaluated on trafﬁc ﬂow

data available in PeMS.

In section II, we describe the problem deﬁnition. In section III, we describe the technical
background of the proposed model. In section IV, a deep learning model, Spatial-DEC, is
proposed for spatio-temporal clustering. In Section V the models are evaluated on trafﬁc ﬂow
data. Section VI describes the conclusions and future works.

II. PROBLEM DEFINITION

Spatio-temporal data is represented with a three dimensional matrix X ∈ Rs,t,f, where s is the
number of sensors, t is the number of time stamps and f is the number of trafﬁc features,
including ﬂow, speed and occupancy. Each ﬁxed location has its own multi-variate time series
data xi ∈ Rt,f. A sliding window method, given a time window w, generates a sequence of data
points. In other words, the function slidingWindow(., .) receives input data X and time window
size w, and outputs data points Xd, which consists of all data points, time series segments, at time
i ∈ Rw,f. Throughout the paper, we represent each data
stamp t and location i, represented with xt
point with two indices i for location index and t for temporal index. A clustering method assigns
a data point xt
i into a cluster cj, where j ∈ {1, . . . , c}, and c is the given number of clusters.
While in alternative approaches, one can consider the problem of clustering of the whole time
series xi ∈ Rt,f, e.g. clustering of trajectory data [45], or sub sequences of spatio-temporal data
xt ∈ Rs,w,f. A clustering model ﬁnds similar data points based on a distance function, such as
euclidean distance. Here, we deﬁne a temporal cluster, when its members have high temporal
similarity, which can be obtained with a DTW distance function. It is desirable to have a more
dense and compact temporal cluster. We deﬁne a spatial cluster, which includes location indices
where their data points have high temporal similarity.

5

Fig. 1: An example of trafﬁc ﬂow data for three road segments, represented with three colors.
The time stamps are every 5-min and the ﬁgure represents trafﬁc ﬂow data for three days.

Trafﬁc ﬂow data is a spatio-temporal data. In Fig. 1, we represent an example of trafﬁc ﬂow for
three sensors and three days. Finding temporal similarity among road networks is challenging
with point-wise clustering. To prevent from the ﬂuctuations in the clusters, we consider segment-
wise clustering. In Fig 2, we describe a schematic representation of the input and output of the
spatio-temporal clustering. The input data points xt
i are time series segments for location i, three
road segments, and time stamp t, two time stamps. The selected data points are from PeMS
trafﬁc ﬂow data, but the time stamps and road segments are arbitrary and are presented with the
purpose of clariﬁcation of the problem deﬁnition. Each data point is a time series of length 12
in the ﬁgure. For 5-min time stamps, each data point represents one hour of trafﬁc ﬂow data for
one road segment. The horizontal axis is time stamps, and vertical axis is trafﬁc ﬂow, normalized
in range [−1, +1]. The three output clusters represents similar data points. The clusters represent
similar patterns over different days and hours. They also represents the locations that are similar
on a transportation networks.

This spatio-temporal clustering problem is challenging. First, the clustering method should
consider both temporal and spatial similarities. Second, given a large number of time stamps,
e.g. six months, t and a large number of road segments of a city s, a sliding window method
generates t × s of data points, which can be a very large number of data points. While k-means
clustering methods have been proposed for time series segments, their performance drops when
they faced with large number of data points, and it has expensive computational time. Moreover,
a k-means clustering method has some limitations to be modiﬁed and consider both spatial and
temporal similarities. Hence, in this work, we propose a deep learning model, Spatial-DEC, to
solve spatio-temporal clustering problem.

A. Autoencoders

III. TECHNICAL BACKGROUND

An autoencoder is primarily proposed in [46]. It consists of an encoder φ(x) = σ(drop(x)w1 +
b1) and a decoder θ(z) = σ(drop(z)w2 + b2), where the activation function and the dropout
function are represented with σ(.) and drop(.), respectively. An encoder is the ﬁrst neural network
component, which reduces the dimension of input data to a latent feature space z ∈ Rd, where
d < n. The second neural network component is a decoder, which reconstructs the input data
from its latent representation.

6

input data points

xt
1

xt+1
1

xt
2

xt+1
2

xt
3

xt+1
3

Spatio-temporal clustering

Cluster 1

Cluster 2

Cluster 3

xt+1
3

xt
1

xt
2

xt+1
1

xt+1
2

xt
3

Fig. 2: An example of spatio-temporal clustering of trafﬁc ﬂow data

In a deep autoencdoer, the encoder and decoder consist of several layers. The encoder and
decoder are a symmetric and multi-layered neural network. The loss function, e.g. mean square
error, reduces the difference of input data and its reconstruction. In other words, the input and
target data are both Xd. For the given spatio-temporal data, the reconstruction loss function is
as follows,

t
(cid:88)

s
(cid:88)

t=1

i=1

1
2

||yt

i − θ(φ(xt

i ))||2

(1)

where t is the number of time stamps, and s is the number of sensors or locations. Also, yt
i is
the target of an autoencoder, which is the same as input data xt
i . Minimization of this objective
function results in learning the latent feature representation of input data. We consider weight
of α2 for reconstruction loss throughout the paper, and in our representation of autoencoders the
weight is α2 = 1.

B. Deep Embedded Clustering

A Deep embedded clustering neural network is introduced in [37]. The encoder transforms x
into latent feature space z. The clustering layer is connected to latent feature layer. The weights
of clustering layer are initialized with cluster centers obtained by k-means clustering. Cluster

7

center j is represented with µj ∈ Rd. Given k as the number of clusters, and d as latent feature
size, the clustering layer is represented with a dense layer Rd → Rk. In other words, it converts
latent features into a vector of size k, which j-th element represents the probability that the data
point is assigned to the cluster j.
Given initial cluster centers {µ1, . . . , µk}, obtained by k-means clustering, and latent features z,
a student’s t-distribution measures the similarity between cluster centers µj and data points xi as
follows,

qij =

(1 + ||zi − µj||2)−1
k(1 + ||zi − µk||2)−1

(cid:80)

(2)

i to a cluster with center point µj is represented with qt

where the degree of freedom of the student’s t-distribution is one. The probability of assigning
a data point xt
ij. The assigned cluster is
argmaxj qt
ij. The clustering algorithm iteratively adjusts clusters by learning from high conﬁdence
assignments. To learn from high conﬁdence assignments, an auxiliary target distribution pij is as
follows,

pt
ij =

(qt
ij)2/fj
k(qt

ij)2/fk

(cid:80)

,

(3)

where fj is the number of elements in cluster j. KL-divergence loss between qt
the high conﬁdence soft cluster assignment,

ij and pt

ij learns

KL(P||Q) =

(cid:88)

(cid:88)

(cid:88)

t

i

j

pt
ij log

pt
ij
qt
ij

(4)

In [38], they train the DEC with joint learning of clustering loss and reconstruction loss. In joint
training, the loss function of the neural network on spatio-temporal data is as follows,

t
(cid:88)

s
(cid:88)

c
(cid:88)
(

t=1

i=1

j=1

α1pt

ij log

pt
ij
qt
ij

+

α2
2

||yt

i − θ(φ(xt

i )||2)

(5)

where c is the given number of clusters, α2 is the weight of mean square error term and α1 is
the weight of clustering loss term. Minimization of the loss function in equation (5) results in
learning the latent feature representation and the output clusters. The model receives input data
Xd and target data are p for clustering layer and Xd for decoder’s output. For DEC, the value
of α1 and α2 represents the importance of each of loss functions. Higher value of α1 reduces
loss function for clustering, while higher value of α2 better keep the structure of autoencoder’s
latent features [38].

IV. SPATIAL DEEP EMBEDDED CLUSTERING

Here, we describe the proposed method for spatio-temporal clustering of trafﬁc data. Algorithm
1 is the procedure of ﬁnding spatio-temporal clusters on trafﬁc ﬂow data.
A deep embedded clustering (DEC) receives data points, xt
i for all locations i and time stamps t,
represented with Xd. The encoder transforms each data point to its latent feature representation zt
i .

8

Fig. 3: The architecture of the Spatial-DEC. The input data x has target values [z, p, y].

Given the number of clusters k, a k-means clustering on latent feature representations ﬁnds mean
of the clusters for latent features. The mean of a cluster µj is obtained by k-means clustering
and is stored into the clustering layer.

The data points with high temporal similarity are close to each other in the latent feature space,
examined in Fig. 7. Hence, each cluster includes data points with high temporal similarity.
However, not only the clusters should represents data points with high temporal similarity, but
also they should consider data points of spatial neighborhood. If a cluster represent data points of
locations, far from each other or distributed in a geographical area, it is not our desired cluster.
Hence, our objective is to obtain clusters with both temporal similarity and spatial closeness. In
the rest of this section, we describe our modiﬁcation to deep embedded clustering and introduce
Spatial-DEC, its architecture is presented in Fig. 3. The objective is to modify the DEC’s loss
function, so if xi and xj are close (or far from) each other, then their latent representations zi
and zj are also close (or far from) each other.
First, we make the latent feature representations conditional to the prior geographical location.
The proposed model needs the location indices as the input data, because it maps the data points
to latent feature space based on both their time series values and location indices. For a given
s sensors, we generate a one-hot encoding of the locations g ∈ Rs, where the location i has the
input vector g, in which gi = 1 and the rest of the values are zeros. This computation of one-hot
encoding is obtained OnehotEncoding() in the algorithm 1. The S-DEC receives [xt
i , gi] as the
input data. The encoder φ([xt
i . Given the time series
segment and its location the encoder outputs low dimensional representation of the data.

i , gi]) outputs latent feature representations zt

Next, we add a loss function into the DEC, and propose Spatial-DEC (S-DEC), where its
latent features are constructed given prior geographical information. We add spatial loss term,
sl(φ([x, g]), z), to the latent feature layer. The encoder’s input are x and g. The encoder’s output
is φ([x, g]). The encoder’s output in the last training steps is stored in z. In other words, the
model uses z as the target value for the latent feature layer. In Algorithm (1), it is shown that
once the DEC obtains p as the target value for the clustering layer, we also obtains z as the
target value for latent feature layer z.

To implement the new loss function effectively, ﬁrst we change the size of the input data. The
goal is to have input data xt
k for each pair of locations i and k. We repeat each
data point, a row in Xd, s times. The new training data are stored in Xd ∈ Rs×s,w. Moreover, we

i and target data zt

9

i , gi] and its encoder’s output φ(xt

reshape the latent features of the last training step, z. Each block of s rows of the z are repeated
s times. The reshaped latent features are represented with z ∈ Rs×s,w. The Spatial-DEC has Xd
as an input data point, and z as the target for latent feature layer. After changing the size of
input data and target data, for any given [xt
i ), there are target
data zt
k for all k. This modiﬁcation allows the model to control the distance of latent features at
location i with all locations k. The loss function optimizes the distance of zt
i to all previously
obtained latent features at the same time stamp, represented with zt
k for all sensors k. The loss
function should increase (decrease) the distance of zt
k, if location i and k are far from
(close) each other. In the rest, we deﬁne the weight matrix λ which controls the distance of
latent features.
We deﬁne a weight value for the loss function, represented with λik, where it is weight that
represent the distance of two locations i and k. A transportation network can be represented with
a graph, and λ is the adjacency matrix that represents distances of locations. Here we assume
that all locations are on a line without loss of generality. We deﬁne λ ∈ Rs,s as the weight
matrix, which represents the spatial distance of locations, where λik ∈ [−1, 1] represents the
distance of two locations i and k. If the value is close to +1, then two locations i and k are
close each other, and if the value is close to -1, then two locations are far from each other.
Given λi ∈ [−1, 1]1,s as the i-th row of λ, we deﬁne λd
i ∈ [−1, +1]s,s as the diagonal matrix of
the elements of λi, that is all element except diagonal elements are zero. We obtain λ ∈ Rs×s,s
by aggregating λi
d for all locations i on the ﬁrst dimension. The calculation is represented with
LossWeight(.) in the algorithm 1. The spatial loss term is as follows,

i and zt

t
(cid:88)

s
(cid:88)

(i+1)×s
(cid:88)

t=1

i=1

k=i×s

(

λik
2

||φ([xt

i , gi]) − zt

k||2)

(6)

In a back-propagation method, the gradients of the Equation (6) along with the gradient of
autoencoder’s loss function are propagated in the neural network. We only describe the back-
propagation for Equation (6). We refer the reader to [37] for further theoretical analysis of
gradient propagation for DEC. Given sl as the spatial loss function, the gradient of Equation (6)
is as follows,

∂sl
∂zt
i

=

∂( λik

2 ||zt

k||2)

i − zt
∂zt
i

= λik(zt

i − zt
k)

(7)

The model ﬁnds the gradient with respect to the encoder’s output zt
i . In stochastic gradient
descent algorithm, the gradient is propagated to update the weights of the neural network. The
loss function is similar to [42]. They uses pairwise distances among clusters and apply the model
on clustering of images for unsupervised learning.
Here we describe the reason that the value of λik directly affects the structure of latent features
and the clustering model. The encoder’s output for a given input data point xt
i . The value of
encoder’s output for last training step and same time stamp t is stored in zt. Given the data point
at location i, the model considers a target value zt
k for all k. Since the neural network minimizes
the loss function, the value of λik controls the distance zt
k for
i and zt
all k. If λik has a positive value, then the loss value is positive for the distance of zt
k.
Training a neural network with such loss value reduces the distance of latent features of zt
i and
zt
k. On the other hand, a negative value for λik increases the distance of latent features of zt
i and

i given the last estimation of zt

i is zt

Algorithm 1 Spatio-temporal clustering with S-DEC

10

1: procedure CLUSTERING( X = {x1, . . . , xs})
2:

Xd ← slidingWindow(X, w)
g ← OnehotEncoding()
λ = LossWeight()
modelAutoencoder.trainAutoencoder(x = [Xd, g], y = Xd)
z ← modelAutoencoder.predictLatent([Xd, g])
µ ← initialKmeans(z)
modelSDEC ← initializeDEC(µ, modelAutoencoder)
Xd ← resizex(Xd)
for epoch ∈ [1,

. . . , maxEpoch]
z, q, y ← modelSDEC.predictClusters([Xd, g])
z ← resizez(z)
p ← targetDist(q)
for [xb, zb, pb] ∈ [Xd, z, p]

modelSDEC.train(x = [xb, g], y = [zb, pb, xb])

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

do

do
return modelSDEC

(cid:46) Return the trained spatial-DEC model for spatio-temporal clusteirng of

trafﬁc data

zt
k. In section 5.2, we validate the effect of new loss function on a sample data. The weight for
spatial loss function, represented in Equation (6), is α0. In the experimental results, α0, α1 and
α2 are the weights of spatial loss, Equation 6, clustering loss, Equation 4, and reconstruction
loss, Equation 1, respectively.

In algorithm (1), we describe procedure of ﬁnding spatio-temporal clusters. Lines 2-9 are the
preprocessing of a DEC model, which includes pretraining an autoencoder, k-means initialization,
and building a DEC model. In line 5, the pre-training of autoencoder is with loss weight values
of α0 = 1.0, α1 = 0.0 and α2 = 1.0. In line 11, the model ﬁnds value of latent features for the
last pretraining step, and it follows by obtaining z and p as target values. Line 13 is the function
in Equation (3), introduced in DEC. Line 14 generates batch size for input and output data. To
clarify our notation, we illustrate the input arguments of a neural network with x, and output
arguments with y, represented in model.train(x, y).

Lastly, we analyze the computational time of the model. In trafﬁc ﬂow data, we have a large
number of data points, n = s×¯t, where it is the multiplication of the number of location and total
time stamps. A k-means clustering method ﬁnds the clusters in order of O(n2) steps. In each
step, the method requires to calculates DTW distance, where it is O(w2) for time series segments
of w. The DEC model maps time series segments to a lower dimension l. In our experiments, we
consider w = 12 and l = 4. A DEC model ﬁnds k-means initialization by a sub-sample of data
points, and train the model in O(En) steps, where E is the number of epochs. With mini-batch
gradient descent we expect to train the model in less than 100 epochs based on our experiments.
In each step, a DEC model requires to apply back-propagation, where its computational time
depends on size of the neural network.

11

V. EXPERIMENTAL RESULTS

Here we illustrate the results for clustering of trafﬁc ﬂow data. The deep learning model
is implemented with Keras. We use a fully-connected autoneocder with 7 layers. All of the
layers have Relu activation function and dropout rate of 0.2. The number of hidden units are
(8, 8, 128, 4, 128, 8, 8) in seven fully-connected layers. The batch size of 288, one day with 5-min
time stamp, and Adam optimizer are selected.

We compare the performance of three models, k-means on latent features of an autoencoder, DEC
and Spatial-DEC. We also have three loss terms, spatial loss, clustering loss, and reconstruction
loss, with corresponding weights α0, α1 and α2, respectively. For k-means with autoencoder, we
have following weights, α0 = 0, α1 = 0 and α2 = 1. For DEC, we have α0 = 0, α1 = 0.2 and
α2 = 1. For Spatial-DEC, we have α0 = 0.1, α1 = 0.2 and α2 = 1.

A. Trafﬁc data

Trafﬁc ﬂow data are obtained from the PeMS [47]. Trafﬁc ﬂow data is gathered from main-line
loop detector sensors every 30 seconds and aggregated to every 5 minutes. The data are for
US-101 South and, I-280 South and I-680 South highways, in the Bay Area of California, which
includes 26 and 16 mainline sensors, respectively, illustrated in Fig. 4. We represent the average
of values on these selected data. We select the data for the ﬁrst ﬁve months of 2016. The models
are trained on the ﬁrst three months, and evaluated on the next two months. In a preprocessing
step, we re-scale the data into the range of [0, 1], and subtract each time window of size w
from its mean value. A time window of size 12, one hour, is selected. In the model we assume
that sensors are on one line in a highway, and the average of results for these two highways is
presented.

Fig. 4: The main-line loop detector sensors on highways in Bay Area, California. The black
boxes are the sensors on I-680-S and I-280-S, and red boxes are US101-S.

B. Validation of the spatial loss function

In Spatial-DEC, we use the spatial loss function introduced in Equation 6. The loss function
decreases (increases) the latent feature representations, if two locations are close (far from)
each other. Here, we examine the correctness of the model by visualizing the latent feature
representations. We consider the ﬁrst six successive sensors on the highway US101-S.

12

(a)

(b)

Fig. 5: The scatter plot of latent feature representations for the sample data. Fig (a) represents
the latent features of an autoencoder without spatial loss. Fig (b) represents the latent features
with spatial loss of weight α0 = 1, and α0 = 10 in Fig (c).

(c)

We obtain the spatial weights, which represents distances of location on a line. We need any
arbitrary function to obtain weights in following ways. If two locations are close (far), their
weights should be close to +1 (-1). We assume that there are six locations on one line. We
use a distance function of λij = (1 − 2 × | i−j
s+1|) for i (cid:54)= j, and zero for λii. Throughout the
experiment, we notice that it better stabilize the clusters. As an example, the spatial weights
for location one are λ1 = [0, 0.71, 0.42, 0.14. − 0.14, −0.42], and for location two are λ2 =
[0.71, 0, 0.71, 0.42, 0.14, −0.14].

We train the Spatial-DEC on a sample data. Fig. 5 represents the latent features in two dimen-
sional with different values of α0. We set α0 to 0, 1.0, 10.0, in Fig. 5.a, 5.b and 5.c respectively.
With α0 = 0.0, the model is an autoencoder without spatial loss. The data points are scattered in
latent feature space without any prior geographical information. With a higher value of α0 = 1.0,
we train the Spatial-DEC model. The results is in Fig. 5.b, where the order of locations from
1 to 6 is preserved in the latent feature space. For a spatial loss weight of α0 = 10, the latent
features are completely separable. The data points are mapped into latent feature with their
corresponding order of distances, from top left to bottom right. In the rest of implementations,
we use a the weight α0 = 0.1. it can be more comparable with DEC and k-means based on
temporal similarity and it also ﬁnds latent feature with spatial closeness. We also notice that an
early stopping of the deep learning results can prevent from completely separable data points
like Fig 5.c, and results in Fig 5.b.

C. Analysis of temporal clusters

After pretraining the autoencoders, the ﬁrst step in training Spatial-DEC is to initialize the clusters
with k-means clustering. To obtain an appropriate number of clusters, we use an Elbow method,
i.e. the optimum value can be obtained, when the reduction in inertia, as the sum of squares
of data points, becomes linear, represented in Fig. 6, where we ﬁnd 80 as the best number of
clusters.

13

Models

kmeans
kmeans-DTW
DEC
Spatial-DEC

Connectivity dis-

Clustering evaluation (percentage results)
Square
Sum
Error to Mean
of Clusters
0.27
0.28
0.22
0.23

0.19
0.17
0.19
0.45

0.21
0.19
0.18
0.11

connectivity

TABLE I: The comparisons of clustering models

Fig. 6: The Elbow method ﬁnds the best value for the number of clusters represented with a
black vertical line.

To show that latent features are directly related to temporal features, the latent feature repre-
sentation of one sensor’s trafﬁc states is shown for ﬁve weekdays in Fig. 7. A t-distributed
stochastic neighbor embedding (TSNE) [48] method is used for representing latent features in
two dimension, with parameters of 40, 300, 500 for preplexity, number of iterations and learning
rate, respectively. The color of each data point represents the hours of a day. One day is grouped
into 10 colors, for every 2 hours. This visualization of latent features show that the data points
are distinguishable based on their time stamps and latent feature preserve temporal properties of
data.

Fig. 7: TSNE representation of autoencoder’s latent features.

To represent temporal similarity in trafﬁc ﬂow data, Dynamic Time Warping have been broadly
used [49]. In our problem, warping window size can be obtained in the range of 1 to 12, the
size of time series segments, where its smaller value reduces the computational time. Comparing

14

the value of Rand Index based on warping window is a method to obtain the best value of
warping window [50]. We notice that there is not a signiﬁcant change in the clusters, obtained
by kmeans clustering with DTW distance function, when we reduce warping window size from
12 to 6. Hence, we selected six as the best warping window. To show that latent feature space
preserve the temporal distances, for any given data points, the euclidean distance of latent features
is calculated. Also the DTW among their time series is calculated. The correlation between
latent features and dynamic time warping is obtained. For latent feature size from 1 to 10, the
correlation changes from 0.9 to 0.98. The maximum correlation between dynamic time warping
and euclidean distance of latent features is 0.98 with latent feature 4. Hence, we selected a latent
feature of size 4 in our analysis. We also conclude that the latent feature space preserve temporal
similarity of data points.

In the rest, we analyze the clusters obtained by k-means, DEC and Spatial-DEC. Unlike su-
pervised learning, in unsupervised learning there is not a clear approach to evaluate clusters.
Hence, we describe properties that we expect to see in the clusters, and based on them deﬁne
evaluation metrics. We expect to have clusters that they are compact and include data points
with high temporal similarities. A feature-based clustering of time series data can improve the
time series forecasting performance [51]. Our clustering models are feature-based, where the
clustering method is applied on the latent feature representation of data points. Hence, we deﬁne
a temporal similarity measure as follows,

sd
j =

1
|Cj|

(cid:88)

xt
i ∈Cj

dtw(xt

i , ¯µj)

where Cj is the set of members of cluster j, ¯µj the mean of the cluster and xt
i is the element i
of the cluster. We consider the element which its latent feature is the closest to the mean of the
cluster as ¯µ or the mediods of the cluster.

Fig. 8: Comparison of implemented clustering models based on sum of square distance of
clusters.

In Fig. 8, we compare the compactness of the clusters using (cid:80)
j for DTW-time series, which
shows the compactness of clusters based on temporal similarity. A more compact cluster better
represents data points with temporal similarity. We compare the implemented DEC and Spatial-
DEC, where both have similar temporal similarity. It is important to show that while Spatial-DEC
ﬁnds more connected clusters, it does not signiﬁcantly reduces temporal similarity in the clusters.
The other model is k-means, applied on latent feature of time series.

j sd

15

D. Analysis of spatial clusters

Here, we evaluate spatial clusters obtained by Spatial-DEC. We deﬁne a spatial cluster as the
set of locations which all have a similar assigned temporal cluster. In other words, for a given
time stamp t, all locations of one spatial cluster have equal assigned cluster c. In trafﬁc ﬂow
data analysis, a spatial cluster represents road segments with similar trafﬁc ﬂow patterns for
a given time stamp. We deﬁne a connected spatial cluster δt
i , which includes location indices
which have similar assigned cluster in a given time stamp t and they all are neighbors. We
deﬁne spatial connectivity as a evaluation metric for the analysis of spatial clusters. A spatial
connectivity shows the total size of connected spatial clusters. If the size of connected spatial
cluster of location i is one, then it means that the assigned cluster of location i is not equal to
the assigned cluster of its neighbors. Such a clustering output is not desirable, as it cannot show
similarity of locations. A desired spatial cluster should have high temporal similarity and spatial
connectivity. We also mention that a high spatial connectivity can reduce temporal similarity,
because the cluster includes larger road segments, which can have lower temporal similarity. A
spatial connectivity is deﬁned as follows,

sc =

t
(cid:88)

s
(cid:88)

t=1

i=1

|δt
i |

(8)

where |δ| is the size of connected spatial cluster.

On the other hand, if a spatial cluster includes location indices, dis-connected in a geographical
area, then the cluster is not desirable. We deﬁne a evaluation metric spatial dis-connectivity
t
as follows. For each location i and time stamp t, we deﬁne δ
i , as the set of location indices
which have an equal temporal cluster to ct
i . The spatial dis-connectivity
is obtained as follows.

i , but they are not in δt

sd =

t
(cid:88)

s
(cid:88)

t=1

i=1

|δ

t
i |

(9)

Fig 9 represents the spatial connectivity and dis-connectivity of the obtained clusters. Higher
value of connectivity shows that data points of closer locations are assigned into same temporal
clusters, which is a more desired cluster. On the other hand, lower value of dis-connectivity
represents that the clusters are not dis-connected in a geographical area. The ﬁgure shows
that Spatial-DEC can signiﬁcantly increase connectivity, and decreases the dis-connectivity of
clusters.

Overall, Fig. 9 shows that the clusters of Spatial-DEC are more compact than k-means in terms
of temporal similarity. In other words, all the data points of one cluster have higher temporal
similarity.
We deﬁne spatial metric as sm = sc − sd. Here, we represent that the mean of st
m for all t
obtained by Spatial-DEC is signiﬁcantly higher than DEC. The null hypothesis H0 is that the
mean of Spatial-DEC and DEC is equal. The alternate hypothesis Ha is that their mean is not
equal. Since, sm can be represented with a normal distribution with positive mean for both DEC
and Spatial-DEC. We apply a t-test on sm obtained by DEC and Spatial-DEC. The p-value is
0.0012, where we can reject the null-hypothesis with signiﬁcant level of α = 0.05. It shows that
the increase in connectivity of spatial clusters is statistically signiﬁcant.

16

Fig. 9: Comparison of implemented clustering models based spatial connectivity and dis-
connectivity.

E. Analysis of trafﬁc ﬂow clusters

Here we visualize and further analyze clusters of trafﬁc ﬂow data. Spatio-temporal clusters shows
that how road segments are similar over time periods, represented in Fig 10. The ﬁgure shows
time stamps for one day on y-axis and location indices on x-axis. Each color represents the
assigned cluster. To better visualize clusters and represents their similarities, we only consider 8
clusters. The areas with same colors have temporal similarity. The ﬁgure shows how 26 locations
are similar over time periods.

Fig. 10: Spatio-temporal clusters obtained by Spatial-DEC

The above representations shows spatio-temporal and spatial clusters. Our clusters are based on
temporal similarity. If a data point is far from the mean of clusters, it means that the data point
is rarely occur in temporal domain, or it is an anomaly. Here, we visualize such an example
to clarify this interpretation. In Fig 11, the heatmap of distance of data points from the center
of clusters is represented and we can see that a portion of values are far form the centers.
Time stamps close to 45 and the ﬁrst 4 locations have light values. Regardless of the reason
of anomaly, we look at trafﬁc ﬂow values in Fig. 12, for the ﬁrst four location. The area close
to time stamp 45 includes a big reduction in trafﬁc ﬂow values. This could be the result of
an accident; however, in this paper, we do not analyze the reasons behind anomalies and the
performance of anomaly detection. These analysis shows different application and importance
of having spatial, temporal and spatio-temporal clusters in a transportation network.

17

Fig. 11: The heatmap of distance of data points to the center of their assigned clusters. The light
colors represents data points far from center of clusters and potential anomalies in data.

Fig. 12: Representation of trafﬁc ﬂow for the ﬁrst four selected sensors. The anomaly is
represented with a box, as there is a big reduction in the ﬂow.

VI. CONCLUSION AND FUTURE WORK

A spatio-temporal clustering is an important method for transportation systems. One of the
challenging problems is to ﬁnd spatio-temporal similarities in a transportation network. To obtain
these similarities in trafﬁc ﬂow data, the problem deﬁnition is represented in Section II. Finding
dynamic clusters of locations in a transportation network, illustrated in Section V.E, is necessary
to analyze trafﬁc congestion propagation and to improve trafﬁc ﬂow prediction and missing data
imputation. Moreover, ﬁnding temporal patterns in trafﬁc ﬂow data is a method for more efﬁcient
prediction and detection of anomalies, illustrated in Section V.E. While these applications are
important in transportation systems, there are few studies in the literature to develop deep learning
models for spatio-temporal clustering of trafﬁc ﬂow data.

Increasing in the availability of trafﬁc data requires further development of clustering models
for complex and high-dimensional data. In this paper, We propose Spatial-DEC, a variation of
Deep Embedded Clustering, to obtain spatio-temporal clusters, and illustrate its performance for
ﬁnding dense and compact temporal clusters in Section V.C and spatially connected clusters
in V.D. The contributions of this work are both in model architecture examined its validity in
Section V.B and deﬁning evaluation metrics for spatial and temporal clusters in Section V.C and
V.D. The proposed model uses the loss function introduced in Eq. 6, and ﬁnds spatio-temporal
clusters. Such a model can be useful not only for trafﬁc data, but also for other spatio-temporal
problems, such as environmental science and smart cities domains. Also, we consider a graph-
structure for latent feature representation, which can be further studied in development of deep
learning models for spatio-temporal data.

18

REFERENCES

[1] G. Atluri, A. Karpatne, and V. Kumar, “Spatio-temporal data mining: A survey of problems and methods,” ACM Computing

Surveys (CSUR), vol. 51, no. 4, p. 83, 2018.

[2] A. M. Nagy and V. Simon, “Survey on trafﬁc prediction in smart cities,” Pervasive and Mobile Computing, vol. 50,

pp. 148–163, 2018.

[3] M. Chowdhury, A. Apon, and K. Dey, Data analytics for intelligent transportation systems. Elsevier, 2017.
[4] F. Rempe, G. Huber, and K. Bogenberger, “Spatio-temporal congestion patterns in urban trafﬁc networks,” Transportation

Research Procedia, vol. 15, pp. 513–524, 2016.

[5] D. Zang, J. Ling, Z. Wei, K. Tang, and J. Cheng, “Long-term trafﬁc speed prediction based on multiscale spatio-temporal
feature learning network,” IEEE Transactions on Intelligent Transportation Systems, vol. 20, no. 10, pp. 3700–3709, 2018.
[6] R. Asadi and A. C. Regan, “A spatio-temporal decomposition based deep neural network for time series forecasting,”

Applied Soft Computing, vol. 87, p. 105963, 2020.

[7] B. Anbaroglu, B. Heydecker, and T. Cheng, “Spatio-temporal clustering for non-recurrent trafﬁc congestion detection on

urban road networks,” Transportation Research Part C: Emerging Technologies, vol. 48, pp. 47–65, 2014.

[8] E. Toch, B. Lerner, E. Ben-Zion, and I. Ben-Gal, “Analyzing large-scale human mobility data: a survey of machine learning

methods and applications,” Knowledge and Information Systems, vol. 58, no. 3, pp. 501–523, 2019.

[9] L. Zhu, F. R. Yu, Y. Wang, B. Ning, and T. Tang, “Big data analytics in intelligent transportation systems: A survey,”

IEEE Transactions on Intelligent Transportation Systems, vol. 20, no. 1, pp. 383–398, 2018.

[10] C. Wang, X. Li, X. Zhou, A. Wang, and N. Nedjah, “Soft computing in big data intelligent transportation systems,” Applied

Soft Computing, vol. 38, pp. 1099–1108, 2016.

[11] S. Wang, J. Cao, and P. S. Yu, “Deep learning for spatio-temporal data mining: A survey,” arXiv preprint arXiv:1906.04928,

2019.

[12] D. Ma, X. Song, and P. Li, “Daily trafﬁc ﬂow forecasting through a contextual convolutional recurrent neural network

modeling inter-and intra-day trafﬁc patterns,” IEEE Transactions on Intelligent Transportation Systems, 2020.

[13] Y. Chen, Y. Lv, and F.-Y. Wang, “Trafﬁc ﬂow imputation using parallel data and generative adversarial networks,” IEEE

Transactions on Intelligent Transportation Systems, 2019.

[14] M. F. Dixon, N. G. Polson, and V. O. Sokolov, “Deep learning for spatio-temporal modeling: Dynamic trafﬁc ﬂows and

high frequency trading,” Applied Stochastic Models in Business and Industry, vol. 35, no. 3, pp. 788–807, 2019.

[15] Z. Cheng, W. Wang, J. Lu, and X. Xing, “Classifying the trafﬁc state of urban expressways: A machine-learning approach,”

Transportation Research Part A: Policy and Practice, 2018.

[16] H. B. Celikoglu, “Dynamic classiﬁcation of trafﬁc ﬂow patterns simulated by a switching multimode discrete cell
transmission model,” IEEE Transactions on Intelligent Transportation Systems, vol. 15, no. 6, pp. 2539–2550, 2014.
[17] W. Wei, Q. Peng, L. Liu, J. Liu, B. Zhang, and C. Han, “Spatio-temporal autocorrelation-based clustering analysis for trafﬁc
condition: A case study of road network in beijing,” in Green, Smart and Connected Transportation Systems, pp. 645–659,
Springer, 2020.

[18] Y. Shi, M. Deng, J. Gong, C.-T. Lu, X. Yang, and H. Liu, “Detection of clusters in trafﬁc networks based on spatio-temporal

ﬂow modeling,” Transactions in GIS, vol. 23, no. 2, pp. 312–333, 2019.

[19] B. S. Kim, B. G. Kang, S. H. Choi, and T. G. Kim, “Data modeling versus simulation modeling in the big data era: Case

study of a greenhouse control system,” Simulation, vol. 93, no. 7, pp. 579–594, 2017.

[20] Y. Cheng, Y. Zhang, J. Hu, and L. Li, “Mining for similarities in urban trafﬁc ﬂow using wavelets,” in 2007 IEEE Intelligent

Transportation Systems Conference, pp. 119–124, IEEE, 2007.

[21] H. Chunchun, L. Nianxue, Y. Xiaohong, and S. Wenzhong, “Trafﬁc ﬂow data mining and evaluation based on fuzzy

clustering techniques.,” International Journal of Fuzzy Systems, vol. 13, no. 4, 2011.

[22] T. T. Nguyen, P. Krishnakumari, S. C. Calvert, H. L. Vu, and H. Van Lint, “Feature extraction and clustering analysis of

highway congestion,” Transportation Research Part C: Emerging Technologies, vol. 100, pp. 238–258, 2019.

[23] J. Tang, L. Li, Z. Hu, and F. Liu, “Short-term trafﬁc ﬂow prediction considering spatio-temporal correlation: A hybrid
model combing type-2 fuzzy c-means and artiﬁcial neural network,” IEEE Access, vol. 7, pp. 101009–101018, 2019.
[24] W. C. Ku, G. R. Jagadeesh, A. Prakash, and T. Srikanthan, “A clustering-based approach for data-driven imputation of
missing trafﬁc data,” in 2016 IEEE Forum on Integrated and Sustainable Transportation Systems (FISTS), pp. 1–6, IEEE,
2016.

[25] X. Qiu and Y. Zhang, “A trafﬁc speed imputation method based on self-adaption and clustering,” in 2019 IEEE 4th

International Conference on Big Data Analytics (ICBDA), pp. 26–31, IEEE, 2019.

[26] A. Salamanis, G. Margaritis, D. D. Kehagias, G. Matzoulas, and D. Tzovaras, “Identifying patterns under both normal
and abnormal trafﬁc conditions for short-term trafﬁc prediction,” Transportation research procedia, vol. 22, pp. 665–674,
2017.

[27] S. Aghabozorgi, A. S. Shirkhorshidi, and T. Y. Wah, “Time-series clustering–a decade review,” Information Systems, vol. 53,

pp. 16–38, 2015.

[28] S. Soheily-Khah, A. Douzal-Chouakria, and E. Gaussier, “Generalized k-means-based clustering for temporal data under

weighted and kernel time warp,” Pattern Recognition Letters, vol. 75, pp. 63–69, 2016.

[29] J. Paparrizos and L. Gravano, “Fast and accurate time-series clustering,” ACM Transactions on Database Systems (TODS),

vol. 42, no. 2, p. 8, 2017.

19

[30] X. Huang, Y. Ye, L. Xiong, R. Y. Lau, N. Jiang, and S. Wang, “Time series k-means: A new k-means type smooth subspace

clustering for time series data,” Information Sciences, vol. 367, pp. 1–13, 2016.

[31] D. Birant and A. Kut, “St-dbscan: An algorithm for clustering spatial–temporal data,” Data & Knowledge Engineering,

vol. 60, no. 1, pp. 208–221, 2007.

[32] X. Yao, D. Zhu, Y. Gao, L. Wu, P. Zhang, and Y. Liu, “A stepwise spatio-temporal ﬂow clustering method for discovering

mobility trends,” Ieee Access, vol. 6, pp. 44666–44675, 2018.

[33] Z. Zhou, J. Yu, Z. Guo, and Y. Liu, “Visual exploration of urban functions via spatio-temporal taxi od data,” Journal of

Visual Languages & Computing, vol. 48, pp. 169–177, 2018.

[34] J. Tang, G. Zhang, Y. Wang, H. Wang, and F. Liu, “A hybrid approach to integrate fuzzy c-means based imputation
method with genetic algorithm for missing trafﬁc volume data estimation,” Transportation Research Part C: Emerging
Technologies, vol. 51, pp. 29–40, 2015.

[35] R. Asadi and A. Regan, “A convolution recurrent autoencoder for spatio-temporal missing data imputation,” arXiv preprint

arXiv:1904.12413, 2019.

[36] E. Min, X. Guo, Q. Liu, G. Zhang, J. Cui, and J. Long, “A survey of clustering with deep learning: From the perspective

of network architecture,” IEEE Access, vol. 6, pp. 39501–39514, 2018.

[37] J. Xie, R. Girshick, and A. Farhadi, “Unsupervised deep embedding for clustering analysis,” in International conference

on machine learning, pp. 478–487, 2016.

[38] X. Guo, L. Gao, X. Liu, and J. Yin, “Improved deep embedded clustering with local structure preservation.,” in IJCAI,

pp. 1753–1759, 2017.

[39] B. Yang, X. Fu, N. D. Sidiropoulos, and M. Hong, “Towards k-means-friendly spaces: Simultaneous deep learning and
clustering,” in Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3861–3870, JMLR.
org, 2017.

[40] P. Tzirakis, M. A. Nicolaou, B. Schuller, and S. Zafeiriou, “Time-series clustering with jointly learning deep representations,
clusters and temporal boundaries,” in 2019 14th IEEE International Conference on Automatic Face & Gesture Recognition
(FG 2019), pp. 1–5, IEEE, 2019.

[41] D. Mautz, C. Plant, and C. B¨ohm, “Deep embedded cluster tree,” in 2019 IEEE International Conference on Data Mining

(ICDM), pp. 1258–1263, 2019.

[42] Y. Ren, K. Hu, X. Dai, L. Pan, S. C. Hoi, and Z. Xu, “Semi-supervised deep embedded clustering,” Neurocomputing,

vol. 325, pp. 121–130, 2019.

[43] N. S. Madiraju, S. M. Sadat, D. Fisher, and H. Karimabadi, “Deep temporal clustering: Fully unsupervised learning of

time-domain features,” arXiv preprint arXiv:1802.01059, 2018.

[44] R. Asadi and A. Regan, “Spatio-temporal clustering of trafﬁc data with deep embedded clustering,” in Proceedings of the

3rd ACM SIGSPATIAL International Workshop on Prediction of Human Mobility, pp. 45–52, 2019.

[45] B. Sabarish, R. Karthi, and T. Gireeshkumar, “Clustering of trajectory data using hierarchical approaches,” in Computational

Vision and Bio Inspired Computing, pp. 215–226, Springer, 2018.

[46] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol, “Stacked denoising autoencoders: Learning useful
representations in a deep network with a local denoising criterion,” Journal of machine learning research, vol. 11, no. Dec,
pp. 3371–3408, 2010.

[47] “California. pems, http://pems.dot.ca.gov/, 2017,” 2017.
[48] L. v. d. Maaten and G. Hinton, “Visualizing data using t-sne,” Journal of machine learning research, vol. 9, no. Nov,

pp. 2579–2605, 2008.

[49] M. Lv, Z. Hong, L. Chen, T. Chen, T. Zhu, and S. Ji, “Temporal multi-graph convolutional network for trafﬁc ﬂow

prediction,” IEEE Transactions on Intelligent Transportation Systems, 2020.

[50] H. A. Dau, D. F. Silva, F. Petitjean, G. Forestier, A. Bagnall, A. Mueen, and E. Keogh, “Optimizing dynamic time
warping’s window width for time series data mining applications,” Data mining and knowledge discovery, vol. 32, no. 4,
pp. 1074–1120, 2018.

[51] K. Bandara, C. Bergmeir, and S. Smyl, “Forecasting across time series databases using recurrent neural networks on groups

of similar series: A clustering approach,” Expert Systems with Applications, vol. 140, p. 112896, 2020.

