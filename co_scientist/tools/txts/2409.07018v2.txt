4
2
0
2

v
o
N
3
1

]
E
M

.
t
a
t
s
[

2
v
8
1
0
7
0
.
9
0
4
2
:
v
i
X
r
a

Clustered Factor Analysis for Multivariate

Spatial Data*

Yanxiu Jin1, Tomoya Wakayama2, Renhe Jiang1 and Shonosuke Sugasawa3

1Center for Spatial Information Science, The University of Tokyo

2Graduate School of Economics, The University of Tokyo

3Faculty of Economics, Keio University

Abstract

Factor analysis has been extensively used to reveal the dependence structures among mul-

tivariate variables, offering valuable insight in various fields. However, it cannot incorporate

the spatial heterogeneity that is typically present in spatial data. To address this issue, we intro-

duce an effective method specifically designed to discover the potential dependence structures

in multivariate spatial data. Our approach assumes that spatial locations can be approximately

divided into a finite number of clusters, with locations within the same cluster sharing similar

dependence structures. By leveraging an iterative algorithm that combines spatial clustering

with factor analysis, we simultaneously detect spatial clusters and estimate a unique factor

model for each cluster. The proposed method is evaluated through comprehensive simulation

studies, demonstrating its flexibility. In addition, we apply the proposed method to a dataset of

railway station attributes in the Tokyo metropolitan area, highlighting its practical applicability

and effectiveness in uncovering complex spatial dependencies.

*This version: November 14, 2024

1

 
 
 
 
 
 
Keywords: Spatial dependence, Heterogeneity, Spatial clustering, Factor analysis, K-means

algorithm

1 Introduction

Multivariate statistical analysis refers to a range of statistical methods aimed at investigating

the dependency structure among multiple variables to understand and explain complex datasets.

These approaches are particularly valuable in disciplines where the interactions between vari-

ables are intricate and potentially influenced by spatial relationships, such as environmental

science (B Patil et al., 2020), sociology (Morales et al., 2022), epidemiology (Reinhart and

Chen, 2021), and urban planning (Song et al., 2020). As the number of variables increases,

statistical model performance and data visualization face challenges. Data dimensionality re-

duction techniques address these challenges by simplifying the dataset while retaining as much

important information as possible. These techniques reduce the number of variables either by

feature selection (Abdulwahab et al., 2022; Hancer et al., 2020; Khaire and Dhanalakshmi,

2022) or feature extraction (Fu et al., 2020; Shrestha, 2021). Factor analysis is one representa-

tive method for feature extraction. It identifies a few underlying factors that explain the patterns

of correlations among observed variables. These factors are defined as linear combinations of

the original variables for ease of interpretation and computation.

The exploratory factor analysis (EFA) model excels at reducing data dimensions by assum-

ing that the relationships between observed variables are homogeneous (Johnson and Wichern,

2007; Fabrigar and Wegener, 2011). However, this assumption often has flaws in geographic

data, where spatial heterogeneity plays a significant role. As stated in the “Second Law of

Geography” (Zhu and Turner, 2022), spatial heterogeneity implies that geographic variables

exhibit uncontrolled variance across space, leading to location-specific variations. This means

that the relationships between variables can vary considerably depending on their locations.

Hence, when EFA is performed on multivariate spatial data, it does not capture the spatial

2

heterogeneity of correlations between variables.

Some studies have attempted to consider the factor model under spatial correlation (Dey

et al., 2022; Wang and Wall, 2003; Krupskii et al., 2018). For instance, Wang and Wall (2003)

assumed that a common spatial factor influences observed variables at different locations, and

used Bayesian methods and Markov chain Monte Carlo computational techniques to estimate

parameters and predict the common spatial factor. Although these studies effectively simulate

how spatial proximity influences the observed values, they may not accurately distinguish how

locations inhomogeneously affect the relationships between variables. This could lead to a

neglect of spatial heterogeneity, where significant differences in variable relationships observed

between different geographical locations may exist. Therefore, this limitation may restrict the

applicability of the models, especially to diverse or heterogeneous structural environments.

Another way to consider spatial attributes is to incorporate spatial heterogeneity into the

modeling process, allowing regression coefficients to vary spatially. This includes widely

used techniques in spatial modeling such as geographically weighted regression (Fothering-

ham et al., 2003), spatially clustered regression (Sugasawa and Murakami, 2021), and spatial

cluster detection (Lee et al., 2017). The advantage of these models lies in their ability not only

to handle the spatial correlation between variables but also to reveal the varying strength of

relationships between different geographical regions. Although these existing methods deal

with spatial heterogeneity, they mainly focus on standard regression problems and have not yet

achieved the interpretable dimensionality reduction regression that is the benefit of EFA.

To overcome this issue, by introducing clustering methods and geographic weights into fac-

tor models, we develop spatially clustered factor analysis (SCFA) that combines the treatment

of spatial heterogeneity with the factor model framework. We assume that the samples can be

divided into a finite number of groups and that the underlying structure of the observed variables

in the same groups is consistent. First, we forcibly divide samples into several groups, where

grouping rules can follow geographical proximity or be randomly assigned. Next, to allow for

3

spatial variation in the SCFA, we use indicators representing the group to which each location

belongs and simultaneously estimate both group parameters and factor models. Additionally,

to encourage such spatial clustered structure, we incorporate a penalized likelihood function

proposed by Sugasawa and Murakami (2021), which is based on the hidden Potts model (Potts,

1952). Similar to the Mixture of Factor Analyzers (MFA) model proposed by Ghahramani

and Hinton (1996), SCFA employs an expectation-maximization (EM) algorithm, iteratively

optimizing group assignment and factor model parameters until convergence. However, unlike

MFA, which does not consider spatial information, SCFA explicitly integrates spatial depen-

dencies through geographic weights, enabling it to capture spatial heterogeneity more effec-

tively. We will demonstrate that the SCFA method can be easily implemented through a simple

iterative algorithm similar to the K-means algorithm (MacQueen, 1967), which combines the

existing algorithms of the EFA with straightforward optimization steps for group assignment.

Although grouping methods using indicators (such as the method proposed) have been widely

used in panel data analysis (Ito and Sugasawa, 2023; Wang et al., 2018) and clustered data

(Sugasawa, 2021), no research has ever applied them to factor analysis of multivariate spatial

data as far as we know.

This paper is organized as follows. In Section 2, we introduce the proposed method, de-

tailing the theoretical foundation and computational framework that underpins our approach.

Section 3 presents a comprehensive simulation study to evaluate the performance and accuracy

of the method under various spatial scenarios. In Section 4, we apply the SCFA method to

a real-world dataset, showcasing its practical utility in capturing spatial heterogeneity and the

relationship between factors and variables. Finally, the contributions of this study and potential

future applications are discussed in Section 5.

4

2 Methods

2.1 Review: Exploratory Factor Analysis

In EFA, observed variables (X) are described as a linear combination of fewer unobservable

random variables (F ). The model equation for the ith subject when m unobservable variables

are considered for modeling the p (≥ m) observed variables can be written as,

Xi =

m
(cid:88)

j=1

aijFj + εi

(i = 1, 2, . . . , p),

where Fj (j = 1, 2, . . . , m) denotes the common factor, aij (i = 1, 2, . . . , p, j = 1, 2, . . . , m) is

the factor loading of ith variable on jth factor, A = (aij) is the p × m matrix of factor loadings,

and εi is the specific factor, which is cannot be explained by the m common factors. In compact

notation,

X = AF + ε,

where X = (X1, X2, . . . , Xp)⊤, F = (F1, F2, . . . , Fm)⊤, and ϵ = (ε1, ε2, . . . , εi)⊤.

Under the assumption of the orthogonal factor model (Johnson and Wichern, 2007), the

common factors and specific factors are independent of each other, so that Cov(εi, Fj) = 0;

the common factors are independent normal variables with the mean of 0 and the variance

of 1, and their covariance matrix is the identity matrix Im, that is, F ∼ N (0, Im); the spe-

cial factor εi ∼ N (0, σ2

i ), and the variances are not necessarily equal with Ψ = V ar(ϵ) =

diag(σ2

1, σ2

2, · · · , σ2

p). Based on these assumptions, the observed variables’ variance-covariance

matrix Σ can be represented as follows:

Σ = AA⊤ + Ψ

(1)

Suppose Xi ∼ N (µ, Σ) is a multivariate normal vector, the unknown parameters A and

5

Ψ can be estimated using the maximum likelihood estimation. Further, the number of factor

m can be determined in a data dependent manner by using, for example, Akaike information

criterion (AIC) and Bayesian information criterion (BIC) (e.g. Choi and Jeong, 2019).

2.2

Spatially Clustered Factor Analysis

Assuming each observed sample xi has corresponding location information si (e.g., longi-

tude and latitude). Then, we divide the p samples into G groups and apply the EFA to each

group. The samples belonging to the same group share the same Ai and Ψi. We consider

G as a fixed value for a while, but we will discuss the data-driven selection of G later. We

introduce gi ∈ 1, 2, . . . , G, an unknown group membership variable for the ith location, and

let Ai = Agi and Ψi = Ψgi. Hence, the unknown parameters in the model are the structural

parameters Ai = (A1, A2, . . . , AG)⊤, Ψi = (Ψ1, Ψ2, . . . , ΨG)⊤, and the membership parameter

g = (g1, g2, . . . , gp)⊤.

When considering the membership parameter, it is reasonable to assume that members in

adjacent locations may share the same membership, as the observed data in adjacent locations

may exhibit similar features due to common underlying factors. To promote the formation of

this structure, we introduce the following penalized likelihood function, motivated by the Potts

model (Potts, 1952) and first adopted by Sugasawa and Murakami (2021):

Q(A, Ψ, g) ≡

p
(cid:88)

i=1

log f (xi|Agi, Ψgi) + ϕ

(cid:88)

i<l

wilI(gi = gl).

(2)

This objective function Q(A, Ψ, g) for the maximization problem can be seen as a combina-

tion of the logarithm of the joint probability density function and the weight function. The

joint probability density function is used to observe the likelihood of samples xi following a

multivariate normal distribution Xi ∼ N (0, AgiA⊤
gi

+ Ψgi); the latter term is used to intro-

duce constraints on spatial dependence. wil = w(si, sl) ∈ [0, 1] is a weight function, and ϕ is

6

considered a hyper-parameter that regulates the strength of spatial similarity. Here, w(·, ·) is a

decreasing function of the distance between two points (e.g., exp(−∥si − sj∥2)), reflecting the

idea that the closer the elements are within the same group, the more reasonable it is.

We adopt an iterative algorithm similar to K-means clustering to maximize the objective

function (2).

In this process, it involves continuously updating the membership variable g

and the parameters (A, Ψ) of the factor analysis models. Each step of the algorithm update

is very simple: maximizing the objective function under known g conditions is essentially

equivalent to maximizing the penalized log-likelihood function for each sample within each

group. The detailed steps of the algorithm are shown in Algorithm 1. The function f (Xg, m) in

Algorithm 1 refers to the exploratory factor analysis model. To update the membership value

gi, we only need to calculate the group g corresponding to the maximum penalty likelihood

function for each sample. As long as G is a moderate value, the update process is feasible

with low computational intensity. Regarding the condition of convergence, we monitor the

difference between the current iteration and the previous iteration:

D(k) =

G
(cid:88)

g=1

Tr(|Ψ(k)

g − Ψ(k−1)
g
Tr(Ψ(k−1)
g

)

|)

,

The algorithm should terminate when the difference D(k) is less than the user-specified toler-

ance value δ, which we use δ = 10−6 in the simulation study.

2.3

Selection of tuning parameters

In the proposed method, there are three tuning parameters: G, representing the number

of groups, m, the number of common factors, and ϕ, which controls the strength of spatial

dependence of gis. Given that the choice of ϕ has minimal impact as long as it remains strictly

positive, we suggest setting ϕ = 1 for simplicity, as Sugasawa and Murakami (2021) discussed.

The number of groups, G, can either be predetermined based on prior information about the

7

Algorithm 1 Spatically Clustered Factor Analysis

Set initial values g(0), A(0), Ψ(0) and m;

- For each ith sample, assign initial membership parameter g(0)
- For g =[1,. . . , G], initialize A(0)

g , Ψ(0)

i

g as the p × m zero matrix, denoted 0p×m.

repeat

- Execute exploratory factor analysis for each group to update Ag, Ψg:
for g =[1, . . . , G] do:
g = f (X (k)
g , Ψ(k)

, m)

g

A(k)
end for

- For each ith sample, update the membership variable:

(cid:40)

log f (xi|A(k)

g ; Ψ(k)

g ) + ϕ

g(k+1)
i

= arg max
g∈{1,...,G}

until convergence

p
(cid:80)
l=1; l̸=i

wilI(g = g(k)

l

(cid:41)
.

)

dataset or determined through a data-driven approach (Sugasawa and Murakami, 2021) using

the following information criterion:

IC(G) = −2

n
(cid:88)

i=1

log f (xi | ˆAi; ˆΨi) + cnG (num(A) + num(Ψ)) ,

(3)

where cn is a constant depending on the sample size n, and num(A) and num(Ψ) denote

the number of elements in the matrix A and Ψ, respectively. Specifically, we use cn =

log(n), which leads to a BIC-type criterion. Then, we choose a suitable value of G as ˆG =

arg minG∈{G1,...,GL} IC(G), where G1, . . . , GL are candidates for G.

For the number of common factors, m, it can be determined by prior information or a data-

driven approach, similar to the parameter G. In this paper, m is selected based on the parallel

analysis introduced by Horn (1965). This is a common way to determine how many factors to

extract when performing factor analysis. Unlike the Kaiser criterion (Kaiser, 1960) of retaining

factors with eigenvalues greater than 1, the parallel analysis compares the eigenvalues derived

from data with those generated from randomly simulated datasets of the same size and struc-

8

ture. The rationale is that factors whose eigenvalues exceed the eigenvalues from the simulated

random data represent true underlying factors in the data, rather than noise. Mathematically,

for each factor m, if the eigenvalue λm of the data is greater than the corresponding average

or percentile eigenvalue λsim

m of the simulated data sets, the factor is considered significant

and retained. This approach helps prevent both over-extraction and under-extraction of factors,

balancing accuracy and interpretability.

3 Simulation Study

3.1

Simulation settings

We present simulation studies to illustrate the performance of the SCFA together with non-

spatial EFA under six scenarios for different cluster locations. In all scenarios, for i = 1, . . . , n,

we let si be the two-dimensional vector of location information (longitude and latitude) in the

squared domain [−1, 1]2 ⊂ R2. The spatial locations {si} are grouped into spatial clusters

D = (D1, D2, . . . , DG), with specific definitions provided in the respective scenarios. Recall

that the factor model is formulated as follows:

xi = Agifi + εi,

εi ∼ N (0, σ2Ip),

where fi ∼ N (0, Im) with m < p, and Ai is a p × m coefficient matrix. Here gi denotes the

group membership, that is, gi = g if i ∈ Dg. We set n = 200, p = 10, m = 3 and G = 4 in our

study. Let agjk be the (j, k)-element of Dg, and (ag1k, . . . , agpk) (the kth column vector of Ag)

are generated from N (µgk, τ 2

g ) for g = 1, . . . , 4, where

(µ11, µ12, µ13) = (1, 1, 1),

(µ21, µ22, µ23) = (−1, 0.5, 0.5),

(µ31, µ32, µ33) = (0.5, −1, 0.5),

(µ41, µ42, µ43) = (0.5, 0.5, −1).

9

To fully demonstrate the diversity of spatial distribution, we have considered the following

six scenarios, with an example of each scenario shown in Figure 1.

- Scenario 1: Uniform distributed clusters. The points {si} are uniformly sampled

from [−1, 1]2, and the sampled points are divided into the following four groups:

D1 = {si | si1 > 0, si2 > 0},

D2 = {si | si1 < 0, si2 > 0},

D3 = {si | si1 < 0, si2 < 0},

D4 = {si | si1 > 0, si2 < 0}.

- Scenario 2: Radial expanding clusters. The sample data are grouped based on their

distance from a central point µ = (0, 0), creating concentric rings of data points. The

sample points are assigned to different clusters based on their distance from the center.

D1 = {si | d(si, µ) ≤ r1},

D2 = {si | r1 < d(si, µ) ≤ r2},

D3 = {si | r2 < d(si, µ) ≤ r3},

D4 = {si | d(si, µ) > r3},

where d(si, µ) = (cid:112)(si1 − µ1)2 + (si2 − µ2)2, and r1, r2, r3 are predefined radii that

determine the boundaries of the concentric groups. We set r1 = 0.25, r2 = 0.5, r3 = 0.75

in our study.

- Scenario 3: Gaussian distributed clusters. The sample points are divided into four

domains, each centered around one of the four cluster centers µj ∼ Uniform([−0.5, 0.5])

for j = 1, 2, 3, 4. The sample points si within each domain are generated to follow an

isotropic normal distribution centered around µj with a standard deviation σ. Specifically,

Dj = {si | si ∼ N (µj, σ2), i = 1, . . . , n

4 }, where σ is set as 0.2 in this study.

- Scenario 4: Anisotropic Gaussian distributed clusters. The sample data are initially

generated as scenario 3, then a linear transformation matrix T is applied to the generated

10

data, transforming D as D = D × T . The transformation matrix T is set as





T =




0.6 −0.6

−0.4

0.8


 .

- Scenario 5: Varied Gaussian distributed clusters. Similar to Scenario 3, the sample

points are distributed around four cluster centers µj, but with different variances σj. The

sampled domain Dj = {si

| si ∼ N (µj, σ2

j ), i = 1, . . . , n

4 }.

In this study, we set

σ1 = 0.2, σ2 = 0.15, σ3 = 0.3, and σ4 = 0.06.

- Scenario 6: Unevenly Gaussian distributed clusters. Different from scenario 3, dif-

ferent clusters are assigned uneven numbers of samples in this scenario. The group sizes

are as predefined: D1 = 50, D2 = 40, D3 = 100, and D4 = 10.

3.2 Methods

We applied the SCFA with the spatial dependency strength parameter ϕ = 1 to the simulated

dataset. For the weights wij, we evaluated two distinct cases:

- Five nearest neighbors. For each location si, the five closest locations are assigned a

weight of wij = 1, otherwise wij = 0.

- Exponential weight function For any si and sj, weights are determined by the expo-

nential decay of the distance:

wij = exp (−∥si − sj∥2/0.12).

These two approaches are denoted by -n and -e, respectively.

As a competitor, we employed the EFA as the baseline model. Although it does not con-

sider spatial structure, it is the most common method to identify the underlying structure of the

11

Figure 1: The examples of six scenarios

data. By comparing the performance of both models, we can evaluate how well each approach

identifies the underlying structures in the spatial data. All estimation processes in this study

were carried out using the Python package “FactorAnalyzer” (Persson and Khojasteh, 2021),

which is based on the implementation of the corresponding R package “psych” (William Rev-

elle, 2024). This performs factor analysis, including the estimation of factor loadings, unique

variances, and the common factors themselves.

To evaluate the difference between the performances of SCFA and EFA, we compare the

estimated covariance matrix ˆA ˆA⊤ with the simulated covariance matrix AA⊤ because the co-

variance matrix effectively captures the overall contribution of the factor loadings in explaining

the observed data. Note that the shapes of the loading matrices estimated by SCFA and EFA

differ. The loading matrix estimated by SCFA has the shape G × p × m while that estimated

by EFA has the shape p × m. This difference in shape makes a direct comparison difficult. To

12

resolve this, we consider the loading matrices for all samples. According to the definition of

EFA, all samples share the same loading matrix, resulting in a total loading matrix of the shape

n × p × m. As we proposed in Section 2.2, samples within the same group share the same

structure, so for a sample xi in group g, Agi = Ag. This results in a loading matrix by SCFA

with a shape of n × p × m. This approach ensures that the loading matrices from SCFA and

EFA are comparable.

We repeated 50 experiments of data generation and fitting each method, and calculated the

average value of the following metrics.

1. Frobenius Distance This measures the magnitude of the difference between two matri-

ces by taking the square root of the sum of the absolute squares of their elements. It is

suitable for measuring overall differences between two matrices.

DFrobenius =

n
(cid:88)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

p
(cid:88)

p
(cid:88)

k=1

i=1

j=1

1
n

(cid:12)
(cid:12)(AA⊤)(k)
(cid:12)

ij − ( ˆA ˆA⊤)(k)

ij

2

(cid:12)
(cid:12)
(cid:12)

2. Wasserstein Distance This measures the minimum “workload” required to transform

one matrix into another matrix. It is particularly useful for comparing the overall distri-

bution similarity of two matrices.

DWasserstein =

1
n

n
(cid:88)

k=1

(cid:16)

AA⊤, ˆA ˆA⊤(cid:17)

=

W2

1
n

n
(cid:88)

k=1

inf
X∼AA⊤
Y ∼ ˆA ˆA⊤

(E∥X, Y ∥2)1/2

3. Chebyshev Distance This metric measures the largest absolute difference between the

corresponding elements of two matrices. It focuses on the one with the largest difference

in all dimensions between two matrices.

DChebyshev = max
i,j,k

(cid:12)
(cid:12)(AA⊤)(k)
(cid:12)

ij − ( ˆA ˆA⊤)(k)

ij

(cid:12)
(cid:12)
(cid:12)

13

4. BIC This measures a balance between the goodness of fit and the model’s conciseness.

A smaller BIC value suggests good prediction performance. We used criterion (3) with

cn = log(n), ensuring an BIC-type criterion.

To further refine our evaluation, we considered different initial grouping methods. Specif-

ically, we experimented with random grouping and K-means clustering based on {si}, using

Python package “scikit-learn” (Pedregosa et al., 2011). Thus, we compared the performance

of EFA with various conditional SCFA, including:

• K-means initial grouping + five nearest neighbors spatial weights.

• K-means initial grouping + exponential weight function.

• random initial grouping + five nearest neighbors spatial weights

• random initial grouping + exponential weight function

3.3 Results

The results are presented in Table 1, which shows the performance of models in different

spatial data scenarios. The SCFA is superior to EFA in all scenarios, demonstrating the effec-

tiveness of the SCFA in spatial data. Notably, for data with clearly separated centers between

groups as in Scenarios 1 and 4, the combination between K-means initial grouping method and

SCFA can effectively identify the structure of underlying variables and maximize its advan-

tages. However, in scenarios without well-separated groups, such as radial expanding situa-

tions, obtaining appropriate initial grouping using the K-means method may be challenging. In

this case, the random method for initial grouping may better leverage the capabilities of SCFA.

Also, given the limited improvement in the Chebyshev distance, a distance that focuses on

the maximum absolute difference between corresponding elements of the matrix, the improve-

ment in the SCFA must have been across the entire multidimensional space. Furthermore, the

14

Table 1: Performance comparison of EFA and SCFA across six scenarios

Scenarios
Uniform

Radial
expanding

Gaussian

Models

EFA
SCFA -K-means -n
SCFA -K-means -e
SCFA -random -n
SCFA -random -e
EFA
SCFA -K-means -n
SCFA -K-means -e
SCFA -random -n
SCFA -random -e
EFA
SCFA -K-means -n
SCFA -K-means -e
SCFA -random -n
SCFA -random -e

Anisotropic EFA

Gaussian

Varied

Uneven

SCFA -K-means -n
SCFA -K-means -e
SCFA -random -n
SCFA -random -e
EFA
SCFA -K-means -n
SCFA -K-means -e
SCFA -random -n
SCFA -random -e
EFA
SCFA -K-means -n
SCFA -K-means -e
SCFA -random -n
SCFA -random -e

Frobenius Wasserstein Chebyshev
Distance
Distance
Distance
10.87
8.71
30.66
10.59
7.97
28.59
10.71
8.39
29.77
10.69
8.30
29.48
10.75
8.50
30.06
10.40
7.76
27.49
10.26
7.45
26.62
10.27
7.48
26.72
10.25
7.43
26.55
10.27
7.47
26.69
11.06
8.69
30.60
10.93
8.42
29.82
10.95
8.50
30.05
10.94
8.47
29.96
10.93
8.52
30.10
11.23
8.60
30.27
11.07
8.20
29.14
11.14
8.45
29.86
11.09
8.25
29.29
11.12
8.43
29.81
11.08
8.63
30.32
10.93
8.25
29.25
10.94
8.40
29.67
10.97
8.37
29.57
10.97
8.48
29.88
10.90
8.50
29.86
10.77
8.12
28.82
10.81
8.22
29.09
10.81
8.23
29.12
13.80
8.25
29.18

BIC
8452
3887
4137
5064
4957
8048
4806
4720
4646
4807
8415
4768
4801
5087
5245
8315
4343
4684
4493
4607
8403
4772
4831
5172
5156
8262
4437
4530
4813
4847

15

Figure 2: The grouping results obtained after applying SCFA

AIC values are significantly improved. This, unlike other metrics, measures its goodness as a

regression equation, assuring that it is more appropriate as a statistical model.

Figure 2 shows the grouping results after applying SCFA, demonstrating its ability to ac-

curately capture the potential structure of the data. Due to the effective integration of factor

structure and spatial information by SCFA, it is able to detect spatial heterogeneity in the struc-

ture and form clearly defined groups that reflect the original distribution patterns. However, in

certain clustering configurations (such as radial expanding and unevenly distributed clustering),

SCFA performs slightly weaker when some groups contain few samples. In this case, limited

sample sizes pose a challenge for identifying groups because the model lacks sufficient data to

capture reliable factor structures.

16

4 Application Study

In this section, we apply SCFA to real data. The dataset contains built environment at-

tributes for 1535 stations in the Tokyo metropolitan area (Figure 3), which comes from Open

Street Map1, MLIT (Ministry of Land, Infrastructure, Transport, and Tourism)2 and e-Stat3 pub-

lic datasets. The selection of attributes of the built environment follows our previous work Jin

et al. (2023), namely Residential, Employment, Commerce & Entertainment, Transportation,

and Administration & Public attributes with five categories, each containing five indicators.

Thus, for each station, we have 25-dimensional variable Xi = (Xi1, Xi2, . . . , Xi25). Descrip-

tions of the variables are provided in the appendix.

Figure 3: Tokyo metropolitan area, including Tokyo, Saitama, Chiba, and Kanagawa Prefec-
tures.

1https://download.geofabrik.de/
2https://nlftp.mlit.go.jp/ksj/index.html
3https://www.e-stat.go.jp/en

17

4.1 Experimental setting

First, it is essential to clarify the selection of three key parameters: the number of factors m,

the spatial weight, w in (2), and the number of groups, G. To determine the appropriate number

of factors, we conducted a parallel analysis on the dataset. As shown in Figure 4a, when the

number of factors is set to 6, the eigenvalues of the actual data exceed those of the randomly

simulated datasets, ensuring the flexibility and validity of factor extraction. Regarding the spa-

tial weight, we adopted approaches -n defined in Section 3.2. Additionally, we modified the -e

method to use the Haversine distance instead of the Euclidean distance. With small differences

in latitude and longitude, the small Euclidean distance makes it difficult for the model to con-

verge. It is more appropriate to use Haversine distance that more accurately reflects real-world

spatial relationships. Besides these, we also considered a case of spatial weight based on the

railway’s network topology, denoted as -t. Specifically, the distance between stations is defined

as the shortest path distance (Dijkstra, 1959). Sensitivity analysis on parameter ϕ revealed that

variations within ϕ ranging from 0.75 to 1.25 had minimal impact on grouping results. Hence,

we set ϕ = 1 and explore the number of groups G from the set G ∈ 1, 2, . . . , 10. Under two ini-

tial grouping conditions, we fixed random seeds to maintain consistent initial groupings under

different spatial weight configurations. Using a BIC-type criterion (3), we determined that the

optimal number of groups is G = 4, with topology-based spatial weight being the best choice.

The results are illustrated in Figure 4b.

4.2 Results of Group Identification

Figure 5 shows the geographical distribution of 1535 stations divided into four groups.

These results demonstrate the spatial distribution characteristics of different groups, further val-

idating the effectiveness of SCFA in handling multivariate spatial data. The stations in Group 1

and Group 4 are mainly concentrated in the central Tokyo metropolitan area and its surrounding

areas, showing a distinct spatial clustering phenomenon. In particular, Group 4 clearly captures

18

(a)

(b)

Figure 4: (a) Parallel analysis scree plot for selecting the number of factors. (b) BIC values
as a function of the number of groups (G) for different spatial weight cases (-e, -t, -n) and two
initial grouping strategies (random and K-means).

the core area of the Tokyo metropolitan area, including Tokyo Station, Chiba Station, Yoko-

hama Station, Omiya Station, and other key urban stations. This concentration indicates that

SCFA can capture a high correlation between stations in urban core areas. Stations in core areas

may share similar attributes such as high population density, high concentration of commercial

activities and transportation facilities, which make these stations exhibit similar spatial depen-

dency structures among them. In contrast, stations in Groups 2 and 3 are distributed on the

periphery, reflecting a more dispersed spatial structure. The heterogeneity reflected by these

peripheral stations may be related to the types of areas they serve (such as residential areas,

natural landscapes or transportation junctions). SCFA successfully identifies this heterogeneity

within different spatial regions and categorizes them into different groups. Furthermore, Group

3 exhibits multi-centered spatial distribution characteristics which indicate that SCFA can not

only capture single-center clustering phenomena but also identify more complex patterns of

spatial expansion and multi-centered clustering structures.

19

Figure 5: Station locations of four groups.

4.3 Effective Underlying Structure Discovery

We visualized factor structure diagrams for each group and kept edges with loadings greater

than 0.4 for simplicity. Figure 6 shows the underlying structures of four groups obtained

through SCFA. The factor structures of each group exhibit significant differences, clearly re-

flecting the impact of spatial heterogeneity on the dependency relationships for each factor

model.

- Group 1. The factor structure of Group 1 displays a relatively simple dependency

pattern, dominated by Factor 1 and Factor 2. Factor 1 primarily influences residential at-

tributes, such as resident density (Variable 1), household density (Variable 2), and walk-

ability (Variable 5), indicating that this factor represents urban characteristics related

to high-density convenient residential areas. Factor 2 reflects employment and housing

price attributes, including worker density (Variable 6), company density (Variable 7),

and housing price (Variable 3). This suggests that labor resources and housing market

dynamics play a significant role in this group.

- Group 2. This group presents a more complex structure, with more variables for Factors

1, 2, and 4, reflecting more diverse spatial characteristics. While Factor 1 and Factor

2 again focus on residential attributes, they also have strong loadings on commercial

(e.g., number of restaurants, Variable 13) and transportation variables (e.g., transfer lines,

Variable 17; intersections, Variable 19; bus stops, Variable 20). This combination likely

20

corresponds to suburban areas with a high dependency on transportation and localized

business services, where residential and commercial functions coexist.

- Group 3. Factor 1 of Group 3 influences a wider range of variables, especially those

related to population density, transportation, and commercial attributes. This suggests

that Group 3 may correspond to spatial regions with diversified functional uses. Factor 2

has strong loading in both employment and residential aspects, reflecting the mixed-use

of the area where living areas and work areas are highly integrated.

- Group 4. The factor structure of Group 4 is relatively simpler, with Factor 1 exerting a

strong influence on employment and commercial characteristics, while Factor 2 predom-

inantly affects transportation and commercial variables. This pattern suggests that Group

4 represents the area with a high concentration of commercial activities, potentially in-

dicative of a central business district or a densely developed commercial area.”

Furthermore, there are similarities in the underlying structures among the four groups. For

example, Factor 1 dominates the dependencies of most variables in each group; Variable 9 (IT

sector proportion) and Variable 10 (major company proportion) often jointly exert strong influ-

ences on the same factor. This reveals some global characteristics in the underlying structures,

where even with spatial heterogeneity present, different regions still share certain common pat-

terns. On the one hand, Factor 1 serves as a global dominant factor that continues to govern

dependencies among numerous variables in each group. This may suggest that certain global

variables (such as infrastructure, population density, and economic activities) exhibit common-

alities across different regions within the Tokyo metropolitan area. These variables might simi-

larly affect dependencies between various regions, leading to the importance of factor 1 within

all groups. On the other hand, whether in urban central areas or peripheral regions, maintaining

consistent relationships between Variables 9 and Variable 10 points to potential economic inter-

dependence. High proportions of IT departments and major companies typically indicate that

21

Figure 6: Underlying structures of factors among four groups

an area is a financial hub where technology and business headquarters drive overall economic

vitality. This coordination transcends spatial boundaries to form uniform interactions.

Finally, we compared the performance of EFA and SCFA on real data using AIC and BIC

as evaluation criteria. The results, as presented in Table 2, show that the SCFA outperforms the

EFA in both criteria. This underscores that spatial grouping in the SCFA works effectively in

real data.

Table 2: Comparison of AIC and BIC between EFA and SCFA Models

Models
EFA
SCFA

BIC
76396
70160

AIC
75462
66425

22

5 Conclusion

In this study, we developed a novel spatially clustered factor analysis (SCFA) that aims

to uncover the underlying dependency structures in multivariate spatial data, addressing the

challenges of spatial heterogeneity and complexity in spatial data. Our approach integrates

the clustering method with exploratory factor analysis, allowing us to partition the data into

clusters where locations within the same group share identical dependency structures. This

combination enhances the ability to capture underlying structural differences caused by spatial

heterogeneity, which traditional exploratory factor analysis methods might overlook. Through

comprehensive simulation studies and empirical applications, we have confirmed that SCFA

can effectively capture structural differences caused by spatial heterogeneity in various spatial

distributions, leading to more accurate variable relationships and factor structures.

In addition, SCFA can effectively identify spatial dependency relationships within different

locations as well as global dependencies. This ability is crucial for analyzing intricate data

across regions and scales. By capturing these diverse dependency structures, SCFA enables

researchers to gain deeper insights into interactions among different regions in complex systems

and their influence on the overall structure. Whether in policy making, resource allocation,

or strategic planning, accurately understanding multilevel dependency relationships improves

the precision and efficiency of decision-making. While our method demonstrates improved

performance on spatially heterogeneous data, it is not intended for non-spatial datasets, as it

relies on spatial attributes to capture meaningful patterns. The applicability and superiority of

this approach are observed primarily when the data exhibits spatial heterogeneity. Future work

could further investigate the theoretical conditions under which this method excels, providing

a more rigorous mathematical foundation for its use.

23

Source Code

The source code for Sections 3 and 4 is available at the GitHub repository (https://

github.com/yanxiuJin/Spatially_Clustered_Factor_Analysis).

Acknowledgments

This work was supported by JST SPRING, Grant Number JPMJSP2108, and JSPS KAK-

ENHI Grant Numbers 22J21090, 21H00699 and 24KJ0750.

Appendix

The station environment data used in this study is classified into five categories based on

functional attributes: residential, employment, commercial and entertainment, transportation,

and administrative and public attributes. According to the variables identified in previous stud-

ies (Jin et al., 2023), five variables were selected for each category. Table A1 presents the range

of independent variables and their sources.

24

Table A1: Variables of the station built environment.

Category

Variable

Residential

Variable 1 - resident density

attributes

Variable 2 - household density

Variable 3 - housing price

Variable 4 - resident consumption
Variable 5 - walkability index 1
Variable 6 - worker density

Employment

attributes

Variable 7 - company density

Variable 8 - financial sector proportion

Variable 9 - IT sector proportion

Variable 10 - major company proportion

Commerce and Variable 11 - number of shopping malls

entertainment Variable 12 - number of restaurants

attributes

Variable 13 - number of entertainments

Variable 14 - number of retails

Variable 15 - commercial area

Scope

1.5 km

1.5 km

1.5 km

1.5 km

–

800 m

800 m

800 m

800 m

800 m

800 m

800 m

800 m

800 m

800 m

Data Source

MLIT

e-Stat

MLIT

e-Stat
Real Estate Homepage2
e-Stat

e-Stat

e-Stat

e-Stat

e-Stat

OSM

OSM

OSM

OSM

MLIT

Transportation Variable 16 - number of transfer lines

Station Homepage of JR station

attributes

Variable 17 - passenger load

Station Homepage of JR station

Variable 18 - number of intersections

Variable 19 - number of bicycle parking

Variable 20 - number of bus stops

Administration Variable 21 - green park area

800 m

800 m

800 m

800 m

and public

Variable 22 - number of administration facilities

800 m

attributes

Variable 23 - number of public facilities

Variable 24 - number of education facilities

Variable 25 - number of sports centers

800 m

800 m

800 m

OSM

OSM

OSM

MLIT

OSM

OSM

OSM

MLIT

1The walkability index is an indicator that assesses the convenience of living within walking distance.
2 https://lifullhomes-index.jp/

25

References

Abdulwahab, H. M., S. Ajitha, and M. A. N. Saif (2022). Feature selection techniques in the

context of big data: taxonomy and analysis. Applied Intelligence 52(12), 13568–13613.

B Patil, V. B., S. M. Pinto, T. Govindaraju, V. S. Hebbalu, V. Bhat, and L. N. Kannanur (2020).

Multivariate statistics and water quality index (WQI) approach for geochemical assessment

of groundwater quality—a case study of Kanavi Halla Sub-Basin, Belagavi, India. Environ-

mental Geochemistry and Health 42, 2667–2684.

Choi, I. and H. Jeong (2019). Model selection for factor analysis: Some new criteria and

performance comparisons. Econometric Reviews 38(6), 577–596.

Dey, D., A. Datta, and S. Banerjee (2022). Graphical Gaussian process models for highly

multivariate spatial data. Biometrika 109(4), 993–1014.

Dijkstra, E. W. (1959, December). A note on two problems in connexion with graphs. Nu-

merische Mathematik 1(1), 269–271.

Fabrigar, L. R. and D. T. Wegener (2011). Exploratory Factor Analysis. Oxford University

Press.

Fotheringham, A. S., C. Brunsdon, and M. Charlton (2003). Geographically Weighted Regres-

sion: The Analysis of Spatially Varying Relationships. John Wiley & Sons.

Fu, H., G. Sun, J. Ren, A. Zhang, and X. Jia (2020). Fusion of PCA and segmented-PCA

domain multiscale 2-D-SSA for effective spectral-spatial feature extraction and data classifi-

cation in hyperspectral imagery. IEEE Transactions on Geoscience and Remote Sensing 60,

1–14.

Ghahramani, Z. and G. E. Hinton (1996). The em algorithm for mixtures of factor analyzers.

Technical report, Technical Report CRG-TR-96-1, University of Toronto.

26

Hancer, E., B. Xue, and M. Zhang (2020). A survey on feature selection approaches for clus-

tering. Artificial Intelligence Review 53, 4519–4545.

Horn, J. L. (1965). A rationale and test for the number of factors in factor analysis. Psychome-

trika 30, 179–185.

Ito, T. and S. Sugasawa (2023). Grouped generalized estimating equations for longitudinal data

analysis. Biometrics 79(3), 1868–1879.

Jin, Y., P. Li, Z. Chen, S. Bharule, N. Jia, J. Chen, X. Song, R. Shibasaki, and H. Zhang (2023).

Understanding railway usage behavior with ten million GPS records. Cities 133, 104117.

Johnson, R. A. and D. W. Wichern (2007). Applied Multivariate Statistical Analysis.

Kaiser, H. F. (1960). The Application of Electronic Computers to Factor Analysis. Educational

and Psychological Measurement 20(1), 141–151.

Khaire, U. M. and R. Dhanalakshmi (2022). Stability of feature selection algorithm: A review.

Journal of King Saud University-Computer and Information Sciences 34(4), 1060–1073.

Krupskii, P., R. Huser, and M. G. Genton (2018). Factor Copula Models for Replicated Spatial

Data. Journal of the American Statistical Association 113(521), 467–479.

Lee, J., R. E. Gangnon, and J. Zhu (2017). Cluster detection of spatial regression coefficients.

Statistics in Medicine 36(7), 1118–1133.

MacQueen, J. (1967). Some methods for classification and analysis of multivariate obser-

vations.

In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and

Probability, Volume 1, pp. 281–297. University of California Press.

Morales, D. X., T. F. Beltran, and S. A. Morales (2022). Gender, socioeconomic status, and

COVID-19 vaccine hesitancy in the US: an intersectionality approach. Sociology of health

& illness 44(6), 953–971.

27

Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,

P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,

M. Perrot, and E. Duchesnay (2011). Scikit-learn: Machine Learning in Python. Journal of

Machine Learning Research 12, 2825–2830.

Persson, I. and J. Khojasteh (2021). Python Packages for Exploratory Factor Analysis. Struc-

tural Equation Modeling: A Multidisciplinary Journal 28(6), 983–988.

Potts, R. B. (1952). Some generalized order-disorder transformations. In Mathematical Pro-

ceedings of the Cambridge Philosophical Society, Volume 48, pp. 106–109. Cambridge Uni-

versity Press.

Reinhart, E. and D. L. Chen (2021). Carceral-community epidemiology, structural racism,

and COVID-19 disparities. Proceedings of the National Academy of Sciences 118(21),

e2026577118.

Shrestha, N. (2021). Factor Analysis as a Tool for Survey Analysis. American Journal of

Applied Mathematics and Statistics 9(1), 4–11.

Song, Z., C. Wang, and L. Bergmann (2020). China’s prefectural digital divide: Spatial anal-

ysis and multivariate determinants of ICT diffusion. International Journal of Information

Management 52, 102072.

Sugasawa, S. (2021). Grouped Heterogeneous Mixture Modeling for Clustered Data. Journal

of the American Statistical Association 116(534), 999–1010.

Sugasawa, S. and D. Murakami (2021). Spatially clustered regression. Spatial Statistics 44,

100525.

Wang, F. and M. M. Wall (2003). Generalized common spatial factor model. Biostatistics 4(4),

569–582.

28

Wang, W., P. C. Phillips, and L. Su (2018). Homogeneity pursuit in panel data models: Theory

and application. Journal of Applied Econometrics 33(6), 797–815.

William Revelle (2024). psych: Procedures for Psychological, Psychometric, and Personality

Research. Evanston, Illinois: Northwestern University. R package version 2.4.6.

Zhu, A.-X. and M. Turner (2022). How is the third law of geography different? Annals of

GIS 28(1), 57–67.

29

