{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff266967-d9e0-458d-8122-c4f42f9270f3",
   "metadata": {},
   "source": [
    "# Reducing Hallucinations\n",
    "\n",
    "- https://levelup.gitconnected.com/solving-6-types-of-hallucinations-in-small-llms-a-hands-on-guide-8d15c11650d3"
   ]
  },
  {
   "cell_type": "code",
   "id": "db9a0af8-4bdf-49b2-bb05-628cb15ab7e5",
   "metadata": {},
   "source": [
    "import torch\n",
    "from gait import s_message, u_message\n",
    "from transformers import pipeline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0457e4b5-c882-40b1-9ed1-84cf1933e4f4",
   "metadata": {},
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 for efficient computation\n",
    "    device_map=\"auto\",  # Automatically selects available GPU/CPU\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f5f2b91e-69e4-4566-8df1-297b355d5c2b",
   "metadata": {},
   "source": [
    "def generate_response(system_prompt: str, user_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response from the model based on a system prompt and user prompt.\n",
    "\n",
    "    Parameters:\n",
    "    - system_prompt (str): The instruction or persona for the model (e.g., \"You are a pirate chatbot\").\n",
    "    - user_prompt (str): The actual user query or message to respond to.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated text response from the model.\n",
    "    \"\"\"\n",
    "    # Construct the input message format for the model\n",
    "    messages = [\n",
    "        s_message(system_prompt),\n",
    "        u_message(user_prompt),\n",
    "    ]\n",
    "\n",
    "    # Generate output using the pipeline\n",
    "    outputs = pipe(messages)\n",
    "\n",
    "    # Extract and return the generated text\n",
    "    return outputs[0][\"generated_text\"][-1][\"content\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "05f81569-dbff-4b4c-b897-e2a0a6c684af",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the model once (outside the function) to avoid reloading on each call\n",
    "embedding_model = SentenceTransformer(\"TaylorAI/gte-tiny\")\n",
    "\n",
    "\n",
    "def get_sentence_embedding(sentence: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate an embedding vector for a given sentence using a preloaded SentenceTransformer model.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence (str): The input sentence to encode.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: The sentence embedding as a NumPy array.\n",
    "    \"\"\"\n",
    "    # Encode the sentence into a dense vector using the preloaded model\n",
    "    embedding = embedding_model.encode(\n",
    "        sentence,\n",
    "        # normalize_embeddings=True,\n",
    "    )\n",
    "\n",
    "    return embedding"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "52f8656e-7965-4b50-a666-36e1c2d02a84",
   "metadata": {},
   "source": [
    "# The system prompt sets the behavior or persona of the AI\n",
    "system_prompt = \"You are an AI Chatbot!\"\n",
    "\n",
    "# The user prompt is the actual question or input from the user\n",
    "user_prompt = \"Who discovered Penicillin in 1928?\"\n",
    "\n",
    "# Generate a response from the AI using the system and user prompts\n",
    "response = generate_response(system_prompt, user_prompt)\n",
    "\n",
    "# Print the response returned by the AI\n",
    "print(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e7d55fd0-69ec-42f2-bd88-490784467143",
   "metadata": {},
   "source": [
    "# Our Knowledge Base (5 Documents)\n",
    "documents = [\n",
    "    \"Robert Withering, an English physician and botanist, is known for his study of the foxglove plant and its medicinal properties, particularly its use in treating dropsy (edema).\",\n",
    "    \"The process of fermentation is a metabolic process that produces chemical changes in organic substrates through the action of enzymes. It typically occurs in yeast and bacteria, and also in oxygen-starved muscle cells, as in the case of lactic acid fermentation.\",\n",
    "    \"Sir Alexander Fleming, a Scottish physician and microbiologist, discovered the antibiotic substance penicillin from the mould Penicillium notatum in 1928. This discovery revolutionized medicine.\",\n",
    "    \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It was named after the engineer Gustave Eiffel, whose company designed and built the tower.\",\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3937aec2-4b92-4848-9423-d02eff62e4eb",
   "metadata": {},
   "source": [
    "# Embed the user query\n",
    "user_query = \"Who discovered Penicillin in 1928?\"  # Our test query\n",
    "user_query_embedding = get_sentence_embedding(user_query)\n",
    "\n",
    "# Embed the Knowledge Base\n",
    "document_embeddings = [get_sentence_embedding(doc) for doc in documents]\n",
    "\n",
    "# Find the most relevant document based on embedding similarity (using dot product)\n",
    "similarity_scores = np.dot(document_embeddings, user_query_embedding)\n",
    "\n",
    "# Find the index of the document with the highest score\n",
    "most_relevant_doc_index = np.argmax(similarity_scores)\n",
    "\n",
    "# Retrieve the text of the most relevant document\n",
    "retrieved_context = documents[most_relevant_doc_index]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b586213-42e1-421b-a240-33db58c764db",
   "metadata": {},
   "source": [
    "retrieved_context"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40d8b992-ace4-4578-ae4f-c0b9faac6970",
   "metadata": {},
   "source": [
    "# We explicitly tell the AI to use the context provided\n",
    "rag_system_prompt = f\"\"\"\n",
    "You are an AI Chatbot!\n",
    "Use the following context to answer the user's question accurately.\n",
    "If the context does not contain enough information to answer the question, \n",
    "respond that you don't have sufficient information from the provided context.\n",
    "\n",
    "Context:\n",
    "{retrieved_context}\n",
    "\"\"\"\n",
    "\n",
    "# The user prompt remains the same\n",
    "user_prompt = \"Who discovered Penicillin in 1928?\"\n",
    "\n",
    "# Generate a response using the RAG system prompt and original user prompt\n",
    "rag_response = generate_response(rag_system_prompt, user_prompt)\n",
    "\n",
    "# Print the RAG-enhanced response\n",
    "print(rag_response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56064ae8-331d-4c46-a54f-c465db9afff7",
   "metadata": {},
   "source": [
    "system_prompt = \"You are an AI Chatbot!\"\n",
    "\n",
    "# The user prompt is the actual question or input from the user\n",
    "user_prompt = \"Who is the president of France today?\"\n",
    "\n",
    "# Generate a response from the AI using the system and user prompts\n",
    "response = generate_response(system_prompt, user_prompt)\n",
    "\n",
    "# Print the response returned by the AI\n",
    "print(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "81452c31-3f6b-43f2-9271-cda243708b38",
   "metadata": {},
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def make_query_time_aware(user_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Rewrites the user prompt to include the current date for temporal context.\n",
    "    This is a simplified example targeting specific keywords.\n",
    "\n",
    "    Parameters:\n",
    "    - user_prompt (str): The original user query.\n",
    "\n",
    "    Returns:\n",
    "    - str: The rewritten, time-aware query.\n",
    "    \"\"\"\n",
    "    now = datetime.now()\n",
    "    # Simple replacements - expand this for more temporal keywords\n",
    "    rewritten_prompt = user_prompt.replace(\"today\", now.strftime(\"%B %d, %Y\"))\n",
    "    rewritten_prompt = rewritten_prompt.replace(\"this year\", now.strftime(\"%Y\"))\n",
    "    rewritten_prompt = rewritten_prompt.replace(\"this month\", now.strftime(\"%B %Y\"))\n",
    "    # You might add more complex logic or regex for different temporal phrases\n",
    "    return rewritten_prompt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e80df1f2-f978-4302-8286-e753fa8825c9",
   "metadata": {},
   "source": [
    "system_prompt = \"You are an AI Chatbot!\"\n",
    "\n",
    "# The user prompt is the actual question or input from the user\n",
    "user_prompt = make_query_time_aware(\"Who is the president of France today?\")\n",
    "print(user_prompt)\n",
    "\n",
    "# Generate a response from the AI using the system and user prompts\n",
    "response = generate_response(system_prompt, user_prompt)\n",
    "\n",
    "# Print the response returned by the AI\n",
    "print(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1951f873-0f6b-4c82-873d-bcc7f0aa9cf8",
   "metadata": {},
   "source": [
    "# The system prompt sets the behavior or persona of the AI\n",
    "system_prompt = \"You are an AI Chatbot!\"\n",
    "\n",
    "# The user prompt is the actual question or input from the user\n",
    "user_prompt = \"Explain how photosynthesis works in plants\"\n",
    "\n",
    "# Initializing a list to store multiple generated responses\n",
    "responses = []\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    # Generating Responses\n",
    "    response = generate_response(system_prompt, user_prompt)\n",
    "\n",
    "    # Append the generated response to the list\n",
    "    responses.append(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c7794d2a-0c15-41cf-98ec-d2641b9bb8e9",
   "metadata": {},
   "source": [
    "responses"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a36864e7-65ec-499d-a450-6abeb39e6289",
   "metadata": {},
   "source": [
    "user_query_embedding = get_sentence_embedding(user_query)\n",
    "\n",
    "# Get embeddings for all responses\n",
    "response_embeddings = [get_sentence_embedding(response) for response in responses]\n",
    "\n",
    "# Calculate similarity scores between the query and each response using dot product\n",
    "similarity_scores = np.dot(response_embeddings, user_query_embedding)\n",
    "\n",
    "# Printing sim score\n",
    "print(similarity_scores)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
